\subsubsection{Regression and the Regression Line}
Regression models are statistical tools that provide functions to estimate the relationship between the response variable and one or more independent variables. Regression analysis is widely adopted by data scientists, who use large datasets to build predictive models for trend forecasting. The following paragraphs will introduce linear regression models and demonstrate their usage using the mtcars dataset.\\

\noindent
\textbf{Simple Linear Models}\\
\\Let response variable $\mathbf{Y}$ be a random $n$-vector of responses, explanatory variable $\mathbf{X}$ is an $n \times 2$ matrix whose elements are known values $x_{ij}$ , and $\beta$ is a $2$-vector of unknown parameters $\beta_0$ and $\beta_1$.\\

\noindent
In a simple linear model, we assume that responses $\mathbf{Y} = (Y_1,Y_2,...,Y_n)^T$ are uncorrelated with a common variance $\sigma^2$ and $E(Y_i| x_i) = \beta_0 + \beta_1 x_i$, which is a linear function of the coefficients $\beta_0$ and $\beta_1$, given values $\mathbf{x} = (x_1, x_2, ..., x_n)^T$ of an explanatory variable. Let $X_i = (1, x_i)$ and $\beta = (\beta_0, \beta_1)^T$, then $E(Y_i| x_i) = X_i \beta$. Then these assumptions can be equivalently written in the vector form :


\[\mathrm{E}(\mathbf{Y} | \mathbf{x}) = 
\left( \begin{array}{cc}
1 & x_1\\
1 & x_2\\
\vdots& \vdots\\
1 & x_n
\end{array} \right) 
\left( \begin{array}{cc}
\beta_0 \\
\beta_1
\end{array} \right) = \mathbf{X} \beta, \quad \text{and} \quad
\text{var}(\mathbf{Y} | \mathbf{x}) =
\begin{pmatrix}
\sigma^2 & 0 & \cdots & 0 \\
0 & \sigma^2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma^2
\end{pmatrix} = \sigma^2 \mathbf{1}_n,
\]

\noindent
where $\mathbf{X}$ is the $n \times 2$ design matrix, $\beta$ are unknown parameters, and $\mathbf{1}_n$ is an n-vector of 1's.\\

\noindent
\textbf{Least Squares Estimation}\\

\noindent
In least square estimation, the residual sum of squares (RSS) is a measure of the goodness of fit in a regression model, representing the sum of squared residuals, where residuals are the differences between the response variables $y_i$ and responses generated by the linear regression model $\mathrm{E}(\mathbf{Y}_i | \mathbf{x})$. Therefore, the goal is find values of parameters  $\beta = (\beta_0, \beta_1)^T$ to minimise the RSS, denoted by $\mathrm{Q}$: 

\begin{equation*}
\mathrm{Q} = \sum_{i=1}^{n} [y_i - \mathrm{E} (Y_i | \mathbf{x})]^2 
           = [\mathbf{y}- \mathrm{E} (\mathbf{Y} | \mathbf{x})]^{T} [\mathbf{y}- \mathrm{E} (\mathbf{Y} | \mathbf{x})] 
           = [\mathbf{y}- \mathbf{X} \beta]^{T} [\mathbf{y}- \mathbf{X} \beta],
\end{equation*}

\noindent
where $\mathbf{y}$ is n-vector of response variables and $\mathbf{X}$ is the $n \time 2$ design matrix. Then expand $\mathrm{Q}$ in terms of vectors and matrices, the partial derivative of $Q$ with respect to vector $\beta$ is:

$$\frac{\partial Q}{\partial \beta} = 2(\mathbf{X}^T\mathbf{X}\beta - \mathbf{X}^T\mathbf{y})\cite{GRM_chp3},$$

\noindent
Equating $\frac{\partial Q}{\partial \beta} = \mathbf{0}$, the vector $\hat{\beta}$, the least squares estimate of $\beta$, can be written as:

$$\mathbf{X}^T(\mathbf{y}-\mathbf{X}\hat{\beta})=\mathbf{0}.$$

\noindent 
If $\mathbf{X}$ has full rank $2$, then $(\mathbf{X}^T\mathbf{X})^{-1}$ exists and there is a unique least square estimate of $\beta$ given by:
$$\hat{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.$$