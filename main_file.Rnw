\documentclass{article}
\usepackage{hyperref}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage[a4paper, total={6in, 8in}]{geometry}

\begin{document}

<<setup, echo=FALSE, message=FALSE, warning=FALSE>>=
library(maps)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(lubridate)
library(readxl)
library(forecast)
library(plotly)
library(sf)
library(tmap)
library(gridExtra)
library(igraph)
data(mtcars)
@

\tableofcontents
\newpage 

\section{Introduction}
XXX
\newpage 


\section{Theoretical Foundations of Data Visualisation}
This chapter, "Theoretical Foundations of Data Visualisation," delves deep into the core principles and concepts that serve as the bedrock of this dynamic field. We seek to understand not only the "how" but also the "why" behind the creation of visualisations that captivate and inform.

\subsection{Introduction to Data Visualisation Theory}
In the pursuit of creating effective data visualisations, it is crucial to understand that behind every chart, graph or plot lies a solid theoretical framework. Theoretical underpinnings provide the foundation upon which data visualisation is built, shaping not only how we represent data but also how we perceive, understand, and interpret it. 

\subsubsection{Guiding Principles for Data Representation}
Within this theoretical framework, we encounter a set of guiding principles that dictate how data should be represented visually. These principles encompass fundamental concepts such as:
\begin{itemize}
    \item \textbf{Accuracy}: Data visualisations should accurately reflect the underlying data, minimising distortion or misinterpretation.
    \item \textbf{Simplicity}: The "less is more" principle applies to data visualisation. Simplified visuals often convey information more effectively than cluttered ones.
    \item \textbf{Clarity}: Visualisations should be clear and understandable to the intended audience, avoiding unnecessary complexity.
    \item \textbf{Relevance}: Information presented should be relevant to the message or question being addressed.
    \item \textbf{Consistency}: Visual elements, such as colour coding and labelling, should be used consistently throughout a visualisation.
\end{itemize}

\subsubsection{Theoretical Framework and Visual Perception}
One of the fundamental aspects of data visualisation theory is an understanding of how the human brain perceives visual information. This knowledge is instrumental in designing visualisations that resonate with viewers. It includes considerations like:
\begin{itemize}
    \item \textbf{Gestalt Principles}: The Gestalt principles of visual perception, including proximity, similarity, and continuity, influence how we group and interpret visual elements in a visualisation.
    \item \textbf{Colour Theory}: The effective use of colour, including colour contrasts and harmonies, can enhance the clarity and impact of a visualisation.
    \item \textbf{Cognitive Load}: Minimising the mental effort required to process information is vital.
\end{itemize}

\subsection{Visual Perception and Cognition}
Understanding the intricacies of how humans interpret visual information is pivotal to the art and science of data visualisation. Thus, we explore human visual perception, along with the application of cognitive psychology principles in data visualisation and highlight the crucial role of pre-attentive attributes in shaping our perception of data. 

\subsubsection{Human Visual Perception: Decoding Visual Information}
Human visual perception is a remarkable cognitive process that allows us to decode and make sense of the world around us. When applied to data visualisation, it illuminates how viewers interact with and derive meaning from visual representations of data. Key aspects of human visual perception in the context of data visualisation include:
\begin{itemize}
    \item \textbf{Pattern Recognition}: The human brain excels at recognizing patterns, making it adept at identifying trends, outliers, and relationships in data visualisations.
    \item \textbf{Perceptual Grouping}: We tend to group visually similar elements together, a principle known as perceptual grouping. This informs how we interpret clusters of data points and shapes in a visualisation.
    \item \textbf{Hierarchy of Perception}: Certain visual attributes are processed more quickly and efficiently than others. For example, colour is often processed faster than text, influencing the viewer's attention hierarchy.
\end{itemize}

By harnessing the principles of human visual perception, applying insights from cognitive psychology, and leveraging pre-attentive attributes, data visualisation designers can create visualisations that are not only aesthetically pleasing but also cognitively efficient.	

\subsubsection{Gestalt Principles}
Gestalt psychology principles have long been recognised as fundamental to the field of visual perception and design. Gestalt psychology is a school of thought that emphasises how humans perceive and organise visual information. It posits that the mind seeks to create meaningful patterns and wholes from individual visual elements. The Gestalt principles thus, provide a framework for understanding how viewers naturally group and interpret visual stimuli.\\

Several key Gestalt principles play a pivotal role in shaping our perception of visual information. These principles include: 
\begin{itemize}
    \item \textbf{Proximity}: Elements that are close to each other are perceived as related or belonging to the same group. In data visualisation, proximity can be used to group data points or related information.
    \item \textbf{Similarity}: Elements that share similar visual attributes, such as colour, shape, or size, are perceived as belonging together. Similarity can be harnessed to encode categorical data or highlight relationships. 
    \item \textbf{Continuity}: The human mind tends to perceive continuous patterns or lines as a single entity. In data visualisation, continuity can aid in representing trends or connections between data points.
    \item \textbf{Closure}: Viewers tend to mentally complete incomplete shapes or patterns. Closure can be employed to suggest relationships or connections even when not explicitly shown in the visualisation.
    \item \textbf{Symmetry}: Symmetrical elements are often perceived as more balanced and harmonious. Symmetry can be used for aesthetically pleasing and easily understandable visualisations.
\end{itemize}

\subsubsection{Application of Gestalt Principles in Designing Visualisations}
The application of Gestalt principles in designing data visualisations can lead to more intuitive and effective communication of information. Designers can strategically leverage these principles to:
\begin{itemize}
    \item Group-related data points to enhance clarity and reduce visual clutter.
    \item Use colour or shape to signify meaningful categories or groupings.
    \item Create smooth, continuous lines or paths to guide the viewer's gaze through the visualisation.
    \item Suggest connections or patterns even in complex datasets.
\end{itemize}

\subsection{Data abstraction and Representation}
The transformation of raw data into meaningful representations is a pivotal step in data visualisation. This process, known as data abstraction, involves distilling complex datasets into visual forms that convey insights. In this section, we explore data abstraction, the hierarchies and levels of abstraction in data visualisation, and the critical trade-offs between abstraction and the potential loss of information.

\subsubsection{Data Abstraction: Transforming Raw Data}
Data abstraction is the art of simplifying and structuring raw data into formats that are comprehensible and insightful. It is the bridge that transforms numbers, text, and variables into visual elements that convey patterns, trends, and relationships. Effective data abstraction is at the heart of creating informative data visualisations.

\subsubsection{Hierarchies and Levels of Abstraction}
In data visualisation, abstraction operates on multiple levels of granularity. Hierarchies of abstraction allow us to represent data at varying levels of detail: 
\begin{enumerate}
    \item \textbf{Low-Level Abstraction}: At the lowest level, raw data is preserved in its most detailed form. This might include individual data points, measurements, or unprocessed text.
    \item \textbf{Mid-Level Abstraction}: As we move up the hierarchy, data is grouped or aggregated to provide a broader overview. For example, hourly data points may be aggregated into daily or weekly averages.
    \item \textbf{High-Level Abstraction}: At the highest level, data is represented in a condensed and abstracted form, often as summary statistics or key insights. This level provides a big-picture view.
\end{enumerate}

\subsubsection{Trade-offs Between Abstraction and Information Loss}
While abstraction is essential for simplifying complex data, it comes with trade-offs. Increasing levels of abstraction can lead to information loss, where fine-grained details or outliers are overlooked. It is crucial for data visualisation designers to strike a balance:
\begin{itemize}
    \item \textbf{Clarity vs. Detail}: Increasing abstraction can enhance the clarity of a visualisation but may sacrifice detailed information that is important for certain analytical tasks.
    \item \textbf{Generalisation vs. Specificity}: Abstraction can provide a more generalised view of data, making it accessible to a wider audience. However, it may miss specific nuances that experts may require.
    \item \textbf{Context vs. Precision}: High-level abstraction can provide valuable context, but it may lack the precision needed for precise decision-making.
\end{itemize}
In data visualisation, the art of data abstraction lies in finding the right level of detail that effectively conveys the intended message while minimising the risk of information loss. This balancing act is a critical consideration in the design of informative and meaningful data visualisations.

\subsection{Data Types and Visualisation Techniques}
In the world of data visualisation, understanding the nature of your data is key. Data comes in various types, and selecting the appropriate visualisation technique is contingent upon recognising these distinctions. In this section, we categorise data types, and demonstrate how to match each data type with suitable visualisation techniques.

\subsubsection{Categorisation of Data Types}
Data types can be broadly categorised into four main types: 
\begin{itemize}
    \item \textbf{Nominal data}: nominal data represents categories or labels without any inherent order. Examples include colours, gender categories, and city names. 
    \item \textbf{Ordinal data}: ordinal data implies a meaningful order or ranking among categories but lacks equal intervals between them. Examples include survey responses (eg. “very satisfied”, “satisfied”, “neutral”, “dissatisfied”, “very dissatisfied”)
    \item \textbf{Interval data}: interval data possesses ordered categories with equal intervals between them, but it lacks a true zero point. Temperature is measured in Celsius or Fahrenheit as an example. 
    \item \textbf{Ratio data}: ratio data includes ordered categories with equal intervals and a meaningful zero point. Examples are age, income, and weight. 
\end{itemize}

\subsubsection{Matching Data Types with Appropriate Visualisation Techniques}
Selecting the right visualisation technique for your data type is pivotal to effective communication. Here are some examples of visualisation techniques matches with corresponding data types: 
\begin{itemize}
    \item \textbf{Nominal data}: Techniques such as bar charts and stacked bar charts are effective in displaying categorical information and relative proportions. 
    \item \textbf{Ordinal data}: Ordinal data can be visualised using ordered bar charts, dot plots, or stacked bar charts, which maintain the ranking and order of the categories.
    \item \textbf{Interval data}: Interval data benefits from visualisation methods like line charts, histograms, and box plots, which highlight trends and distributions without assuming a true zero point. 
    \item \textbf{Ratio data}: Ratio data can be effectively presented using scatter plots, histograms, and line charts, allowing for precise comparisons and measurements due to the presence of a meaningful zero point. 
\end{itemize}

\subsection{Colour Theory in Data Visualisation}
Here, we explore the significance of colour in data visualisation, the principles of colour perception and encoding, and the importance of avoiding misleading visualisations through thoughtful colour choices.

\subsubsection{The Importance of Colour in Conveying Information}
Colour is a potent tool for enhancing the understanding and impact of data visualisations. It enables the differentiation of data points, highlights trends, and provides context. Colour can be used to:
\begin{itemize}
    \item \textbf{Encode Categorical Data}: Distinguish between different groups or classes using distinct colours.
    \item \textbf{Represent Quantitative Data}: Use colour intensity or gradients to represent values or magnitudes.
    \item \textbf{Add Context}: Apply colour to background elements, labels, or annotations to provide context and meaning to the visualisation.
\end{itemize}

\subsubsection{Colour Perception and Colour Encoding in Visualisations}
Understanding how colour is perceived by viewers is essential in data visualisation. Key principles include:
\begin{itemize}
    \item \textbf{Colour Discrimination}: Consider that not all viewers may perceive colour in the same way. Ensure your colour choices are accessible to individuals with colour vision deficiencies (colour blindness).
    \item \textbf{Colour Encoding}: Select colour schemes that align with the message you want to convey. For example, warm colours like red and orange often signify caution or heat, while cool colours like blue and green convey calmness or coldness.
    \item \textbf{Colour Combinations}: Pay attention to how colours interact when placed adjacent to each other. Some combinations may create visual vibrations or make text difficult to read.
\end{itemize}

\subsubsection{Avoiding Misleading Visualisations Due to Colour Choices}
Misleading visualisations can result from inappropriate or deceptive use of colour. To avoid this:
\begin{itemize}
    \item \textbf{Consistency}: Maintain consistency in colour usage throughout your visualisation. Use the same colour scheme for similar data categories or elements.
    \item \textbf{Avoid Distortion}: Ensure that colour choices do not exaggerate or distort the data. Overly intense or contrasting colours can lead to misinterpretation.
    \item \textbf{Legend Clarity}:  Provide a clear and concise legend to explain the meaning of colours, especially when dealing with complex or unfamiliar colour schemes. 
    \item \textbf{Test with Users}: Conduct user testing to ensure that your colour choices effectively convey the intended message and do not confuse or mislead the audience.
\end{itemize}

\subsection{Theoretical Properties of Visualisations}
Effective data visualisation is not solely about creating aesthetically pleasing graphics; it's also about adhering to key theoretical properties that optimise the expressiveness, precision, accuracy, and scalability of visual representations. In this section, we delve into these properties, including expressiveness and effectiveness, the data-ink ratio, and the principles of minimal ink, as well as precision, accuracy, and scalability.

\subsubsection{Expressiveness and Effectiveness}
\begin{itemize}
    \item \textbf{Expressiveness}: Visualisations should be expressive, meaning they should effectively communicate the intended message or insights within the data. Expressive visualisations capture the richness and complexity of the underlying data, revealing patterns, trends, and relationships.
    \item \textbf{Effectiveness}: An effective visualisation is one that successfully conveys information to its audience. It allows viewers to understand the data, draw meaningful conclusions, and make informed decisions based on the presented information.
\end{itemize}

\subsubsection{Data-Ink Ratio and the Principle of Minimal Ink} REVISE REFERENCES!!
This \textbf{Data-Ink Ratio principle}, introduced by Edward Tufte, emphasises maximising the ink (or pixels in digital formats) used to represent the actual data while minimising non-essential ink. A higher data-ink ratio results in a cleaner, more efficient visualisation that reduces clutter and enhances comprehension.\\
The \textbf{Principle of Minimal Ink} builds on the data-ink ratio. This principle advocates for the removal of any visual elements that do not contribute to the viewer's understanding of the data. Eliminating unnecessary ink (e.g., excessive gridlines or decorations) simplifies the visualisation without sacrificing its effectiveness.

\subsubsection{Precision, Accuracy, and Scalability}
\begin{itemize}
    \item \textbf{Precision}: Precision in data visualisation refers to the level of detail and granularity in the representation of data. Visualisations should strike a balance between showing enough detail to support accurate interpretation while avoiding overwhelming viewers with excessive complexity.
    \item \textbf{Accuracy}: Accuracy pertains to how faithfully the visualisation represents the true values in the data. Misleading or distorted visualisations can lead to incorrect conclusions. Therefore, maintaining accuracy is essential.
    \item \textbf{Scalability}: Scalability addresses how well a visualisation adapts to different data sizes or resolutions. Effective visualisations should be scalable, and capable of representing both small and large datasets without sacrificing clarity or performance.
\end{itemize}

\subsection{Cognitive Load and Visual Complexity}
In data visualisation, achieving a balance between complexity and cognitive load is crucial. This section explores the concept of cognitive load in visualisations, strategies to reduce cognitive load while maintaining complexity, and techniques to combat information overload through simplification.

\subsubsection{Exploring the Concept of Cognitive Load in Visualisations}
In data visualisations, cognitive load plays a significant role in how viewers engage with and understand the presented data. It is essential to strike a balance that ensures the visualisation conveys information effectively without overwhelming or overtaxing the viewer's cognitive capacity.

\subsubsection{Strategies to Reduce Cognitive Load While Maintaining Complexity}
\begin{itemize}
    \item \textbf{Visual Hierarchy}: Establish a clear visual hierarchy that guides viewers' attention to the most important elements of the visualisation. Use techniques such as size, colour, and contrast to emphasise key information.
    \item \textbf{Simplify Labels and Text}: Reduce cognitive load by using concise labels and text. Avoid jargon and unnecessary complexity in annotations, ensuring that labels are informative and straightforward.
    \item \textbf{Interactive Features}: Implement interactive elements, such as tooltips and drill-down functionality, to provide additional information when viewers need it, reducing the need for dense, static visualisations.
    \item \textbf{Progressive Disclosure}: Present complex information gradually, allowing viewers to digest it in stages. Start with an overview and provide opportunities for users to explore details as needed.
    \item \textbf{Data Aggregation}: Consider aggregating data when it makes sense. Summarising data can reduce the cognitive load associated with interpreting fine-grained details.
\end{itemize}

\subsubsection{Information Overload and Simplification Techniques}
Information overload occurs when a visualisation overwhelms viewers with excessive data or visual elements, hindering comprehension. To combat information overload, the following simplification techniques can be applied:
\begin{itemize}
    \item \textbf{Filtering}: Allow viewers to filter data based on specific criteria, enabling them to focus on the most relevant information.
    \item \textbf{Data Reduction}:  Aggregate or summarise data to present overarching trends or patterns instead of inundating viewers with raw data points.
    \item \textbf{Studyboarding}: Use storytelling techniques to guide viewers through the data in a structured manner, helping them understand the context and relevance of the information presented.
    \item \textbf{Prioritisation}: Identify the most critical information and prioritise its display, relegating less essential data to secondary views or interactions.
\end{itemize}

\newpage

\section{Modern Methods of Data Visualisation}
In this chapter, we explore a variety of powerful visualisation methods, from classic scatter plots and bar charts to advanced techniques like heatmaps and network graphs. Through vivid examples, we'll show when and why each method is used, and delve into the theoretical and mathematical foundations that empower these visualisations to unveil insights hidden within the data.

\subsection{Introduction to Modern Data Visualization Methods}
As data grows increasingly complex and vast, the tools and techniques for effectively conveying this information continue to expand and refine. In this section we introduce the data sets that will be used to illustrate each of the different visualisatio techniques. 

\subsubsection{Data Sets}
\begin{enumerate}
\item \textbf{Mtcars dataset}: The mtcars dataset in R is a built-in dataset that contains information about various car models. It provides data on the characteristics of 32 different car models, which were available in the early 1970s. The dataset includes a total of 11 variables, each representing different attributes of these cars, such as miles per gallon (mpg), horsepower (hp), number of cylinders (cyl), and more. The mtcars dataset is often used for data analysis, visualization, and statistical modeling, making it a useful resource for exploring and practicing data science techniques in R.
\item \textbf{Annual fire in Brazil}: it is obtained from \href{https://firms.modaps.eosdis.nasa.gov/}{NASA}. Each dataset from 2013 to 2022 contains over 200,000 observations. Over the decade, there are more than 3 million observations. The trend is impossible to analyse by eye. However, an exploratory analysis using heatmaps provides insights into this data.
\end{enumerate}

\subsection{Scatter Plots and Bubble Charts}
Scatter plots and bubble charts are fundamental data visualisation techniques that provide valuable insights into the relationships and patterns within datasets. These visualisations are particularly effective for representing discrete data through data points, since this bring out easily identifiable comparisons, and reveals trends.

\subsubsection{Scatter Plots}
Scatter plots, also known as dot charts or dot density plots, offer a straightforward yet mathematically intriguing method for visualizing data. At their core, they display individual data points as dots along a single axis, with each dot representing a single observation. The mathematical interest of dot plots lies in their ability to provide a simple visual representation of data distribution, center, and spread. While they don't rely on complex equations or statistical principles, dot plots make it easy to observe important characteristics of data, such as the mode (the most frequent value), skewness (asymmetry), and potential outliers. They're particularly useful for comparing multiple data sets, identifying patterns, and detecting data irregularities, all while offering an intuitive and accessible way to grasp the fundamental concepts of data visualization and data analysis. This simplicity is what makes dot plots a valuable tool for both introductory statistics education and exploratory data analysis.

\subsubsection{Scatter Plots in Practice}
In this example, we'll create a scatter plot that visualizes the relationship between two variables - the weight of cars and the amount of miles traveled per gallon of petrol. We'll use the "mtcars" R dataset.

<<scatter-plot-chunk, echo=FALSE, fig.height=4, fig.width=6, fig.cap='Scatter plot of car weights vs MPg'>>=
# Create a scatter plot of car weight vs MPG from mtcars dataset
library(ggplot2)
ggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +
  geom_point(size = 4, alpha = 0.7) +  # Larger points with transparency
  geom_smooth(method = "lm", se = FALSE, color = "red", linewidth = 1) +  # Linear regression trend line
  labs(
    x = "Weight (1000 lbs)",
    y = "Miles per Gallon",
    color = "Cylinders"
  ) +
  scale_color_manual(values = c("4" = "grey", "6" = "purple", "8" = "blue")) +  # Custom color palette
  theme_minimal()  # Minimalistic theme
@

\subsubsection{Analysis}
The scatter plot displays the relationship between car weight (in thousands of pounds, "wt") and fuel efficiency (miles per gallon, "mpg") for different car models. While this graph plots two variable, through the use of colour, additional lines, and other subtle details, we can make of this basic graph, a more readable and visually interesting figure which the reader can get easily gather information from. Here are the key characteristics of the plot:
\begin{itemize}
    \item Color-Coded Cylinders: The points are color-coded based on the number of cylinders in the engine (4, 6, and 8 cylinders). This allows for a quick visual differentiation of car types, enhancing the understanding of the data.
    \item Point Size and Transparency: Points have been enlarged and have a subtle degree of transparency for visual appeal. Larger, more prominent points are easier to see, while transparency helps in the visualization of overlapping points.
    \item Linear Regression Line: A red linear regression trend line is included. It provides a visual representation of the overall relationship between car weight and fuel efficiency, indicating a negative correlation—cars tend to have lower fuel efficiency as their weight increases.\\
\end{itemize}

\subsubsection{Regression and the Regression Line}
\textbf{Regression} is a statistical technique used to model and understand the relationship between two or more variables. It is a fundamental tool in data analysis and predictive modeling. In simple linear regression, a linear equation is used to describe the relationship between a dependent variable (the one you want to predict) and one or more independent variables (the predictors).The equation takes the form of \(Y = aX + b\), where \(Y\) is the dependent variable, \(X\) is the independent variable, \(a\) is the slope, and \(b\) is the intercept.\\
Regression analysis calculates the best-fit line that minimizes the sum of squared differences between the predicted and actual values. This line provides valuable insights into the strength and direction of the relationship between variables, allowing us to make predictions and understand how changes in one variable affect another. Adding regression lines to scatter plots can enhance their value by providing a clearer depiction of trends and enabling more accurate predictions, turning the plot into a predictive tool rather than just a visual representation of data points.

\subsubsection{Bubble Charts}
Bubble charts, like scatter plots, are a valuable tool in data visualization that represent data points as bubbles or circles on a two-dimensional plane. They introduce an additional dimension by encoding data using the size of these bubbles. The mathematical intricacies behind bubble charts involve principles of geometry and data scaling.\\
To calculate the size of each bubble, a scaling factor is applied to map the data values to a suitable range for bubble sizes. Typically, linear or nonlinear scaling methods are used to ensure that the bubble sizes accurately reflect the underlying data. The equations used in bubble charts are relatively straightforward, but the challenge lies in selecting appropriate scaling methods and ensuring that the size variations effectively convey the data's significance. 

\subsubsection{Bubble Charts in Practice}
 This bubble plot visualizes data from the same dataset as above. The purpose of this plot is to depict the relationship between car models and their fuel efficiency (mpg) while using the size of the bubbles to represent the car's horsepower (hp) and color-coding the bubbles based on the number of cylinders (cyl).

<<buble-plot-chunk, echo=TRUE, fig.height=5, fig.width=10, fig.caption='Bubble plot illustrating the relationship between car models and miles per gallon'>>=
#Create bubble plot 
ggplot(mtcars, aes(x = rownames(mtcars), y = mpg, size = hp, color = cyl)) +
  geom_point() +
  labs(
    x = "Car Models",
    y = "Miles per Gallon",
    size = "Horsepower (hp)",
    color = "Cylinders (cyl)"
  ) +
  scale_size_continuous(range = c(3, 10)) +
  scale_color_gradient(low = "lightblue", high = "darkblue") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust =1))
@

\subsubsection{Analysis}
The plot's title, axis labels, and legends provide context and clarity to the visualization, making it accessible and informative. Additionally, the choice of a gradient color scale for the number of cylinders enhances the visual appeal and aids in interpreting the data.This bubble plot allows for quick comparisons between multiple characteristics of different car models.The resulting bubble plot effectively conveys several key insights:
\begin{enumerate}
\item \textbf{Car Model vs. MPG}: The x-axis displays the car models, offering a clear representation of each vehicle in the dataset. The bubble plot is particularly useful for displaying nominal data, such as car model names, as it allows easy identification and comparison.
\item \textbf{Miles per Gallon (MPG)}: The y-axis measures miles per gallon, representing the fuel efficiency of each car model. Higher bubbles indicate better fuel efficiency. This variable, which is continuous, is positioned vertically to demonstrate how each car model's fuel efficiency relates to others.
\item \textbf{Horsepower (HP)}: The size of each bubble represents the car's horsepower (hp). Larger bubbles correspond to higher horsepower, providing an additional dimension to the data. The size encoding helps identify more powerful cars.
\item \textbf{Cylinders (Cyl)}: The color of each bubble is determined by the number of cylinders (cyl) in the car's engine. The color scheme adds a categorical aspect to the visualization, making it easy to differentiate between cars with different cylinder counts.
\end{enumerate}


\subsection{Bar Charts and Histograms}
\subsubsection{Bar Charts}
A bar chart is a very important method to present data. It organizes information into vertical bars.  Bar charts have lots of advantages in data visualization. It can present data categories in a frequency distribution. A bar chart is best for comparing classified data. Especially when the values are close, because the human perception of height is better than other visual elements (such as area, angle, etc.), the use of a bar chart is more appropriate. These bars usually have different lengths, and every length is proportional to the size of the information they present.\\
R uses the function $barplot()$ to create bar charts. R can draw both vertical and Horizontal bars in the bar chart. In the bar chart, each of the bars can be given different colors.\\
R is a programming language for data analysis and statistical computing, and its advent has made data visualization more straightforward and accessible. Among the various tools available in R, ggplot2 stands out as one of the most renowned and powerful tools for creating data visualizations. It offers a wealth of data visualization capabilities and is celebrated for its versatility and aesthetic appeal. In this chapter, we will focus on how to use ggplot2 to create bar charts for data visualization.\\

\subsubsection{Different Types of Bar Charts}
Here is an overview of the different types of bar charts. \\
\paragraph{Vertical Bar Chart}
This is the most common bar chart. We use different vertical columns to display and compare the values of different categories in the same dimension, where the X-axis represents the contrasting categories and the Y-axis represents the frequency or count of their categories.\\
\paragraph{Horizontal Bar Chart}
This is very similar to a vertical bar chart but rotated 90 degrees. Categories are shown on the y-axis and frequency or count are shown on the x-axis. Horizontal bar charts are especially useful when category names are long or when there are numerous categories.
\paragraph{Multi-set Bar Chart}
Also known as a grouped bar chart or clustered bar chart. A multi-set bar chart is used to represent and compare different sub-groups within individual categories. This type of chart is useful when you want to show and compare multiple sets of data side-by-side.
Multi-set Bar charts can be horizontal or vertical like the other normal bar charts, and the length of each bar represents the frequency or count of their categories.
\paragraph{Stacked bar chart}
Similar to bar charts, stacked bar charts are often used to compare different classes of values and, within each class of values, are divided into sub-classes, which are often referred to by different colors. Each segment's size is proportional to the frequency or count that it represents from the sub-category. The entire bar's length represents the cumulative total of all the sub-categories.
However, it is very easy to get confused when there are too many categories.

\subsubsection{Advantages of Bar Charts}
\begin{enumerate}
\item \textbf{Clarity and Simplicity}: Bar charts are structurally simple, making them easy to read and understand, allowing audiences to quickly grasp key information.
    \item \textbf{Effective Comparison}: They provide a visual representation that makes comparing the size or value of different categories straightforward, especially when comparing a limited number of categorical data.
    \item \textbf{High Flexibility}: They can be used to represent any type of data, be it continuous or discrete.
    \item \textbf{Multilevel Representation}: Stacked or grouped bar charts can be used to represent multiple data series.
\end{enumerate}

\subsubsection{Disadvantages of Bar Charts}
\begin{enumerate}
    \item \textbf{Limited Data Representation}: They might not be suitable for representing large datasets as things can get cluttered.
    \item \textbf{Potential Misinterpretation}: Without a zero baseline, bar charts can be misleading as they might exaggerate differences.
    \item \textbf{Overcomplexity with Many Categories}: If there are too many bars, it can be challenging to discern information effectively.
    \item \textbf{Requires Categorical Data}: Bar charts are not ideal for representing trends over continuous data, where line graphs might be more appropriate.
\end{enumerate}

In this section, I will analyze the air quality dataset provided by the United States Environmental Protection Agency. In our dataset, we have data from over 200 locations. The Air Quality Index (AQI) ranges from 0 to 500. A higher AQI indicates increased levels of air pollution, leading to heightened health concerns. This implies that as the AQI rises, there is a greater risk to public health.

<<my_plot1, fig=TRUE, echo=TRUE, tidy=TRUE, width.cutoff=60>>=
data <- read.csv("c4_epa_air_quality.csv")
ggplot(data, aes(x = aqi)) +
  geom_bar() +
  labs(title = "Bar Chart Example 1", x =
         "Air Quality Index/units", y = "number/sites") +
  theme_minimal()
@

The above code is not optimal. Upon examinationan, we can see that there is an excussive number of different categories on the x-axis. Consequently, the multitude of vertical bars in the graph can potentially overwhelm and confuse readers. 
In order to solve the problem, we can use the “cut” function to divide the data into intervals of five units each. For instance, values from 0-5 would constitute one group, 6-10 another, 11-15 would form the next group, and so forth.


<<my_plot2, fig=TRUE, echo=TRUE, tidy=TRUE, width.cutoff=60>>=
data <- read.csv("c4_epa_air_quality.csv") #
# load ggplot2 package
library(ggplot2)

# Use the cut function to divide the data into groups of five intervals
data$group <- cut(data$aqi, breaks = 
                    seq(0, max(data$aqi) + 5, by = 5), 
                  right = FALSE, include.lowest = TRUE)
data$group <- as.numeric(data$group) 

# Y-axis representing the number of occurrences of the X-axis label in the data
ggplot(data, aes(x = group)) +
  geom_bar() +
  labs(title = "Bar Chart Example 1", 
       x = "Air Quality Index/units", y = "Number/sites") +
  theme_minimal() +
  scale_x_discrete(labels = scales::label_number(accuracy = 5))

@

We can also add some color to make our plot more attractive, here we can add sone color as well. In the code below, we set the color of the bar chart to blue while specifying the border color as black.

<<my_plot3, fig=TRUE, echo=TRUE, tidy=TRUE, width.cutoff=60>>=
# read CSV dater set
data <- read.csv("c4_epa_air_quality.csv") 

# load ggplot2 
library(ggplot2)

# Use the cut function to divide the data into groups of five intervals
data$group <- cut(data$aqi, breaks = seq(0, max(data$aqi) + 5, by = 5), 
                  right = FALSE, include.lowest = TRUE)
data$group <- as.numeric(data$group) 
# Converts the group column to numeric type

ggplot(data, aes(x = group)) +
  geom_bar(color="black",fill="blue") +
  labs(title = "Bar Chart Example 1", x = "Air Quality Index/units", 
       y = "Number/sites") +
  theme_minimal() +
  scale_x_discrete(labels = scales::label_number(accuracy = 5))
@

Finally, we add lables to the X-axis to define the range of each categories.
<<my_plot4, fig=TRUE, echo=TRUE, tidy=TRUE, width.cutoff=60>>=
# read csv
data <- read.csv("c4_epa_air_quality.csv")

# load ggplot2
library(ggplot2)

# Use the cut function to divide the data into groups of five intervals
breaks_list <- seq(0, max(data$aqi) + 5, by = 5)
data$group <- cut(data$aqi, breaks = breaks_list,
                  right = FALSE, include.lowest = TRUE)


ggplot(data, aes(x = group)) +
  geom_bar(color="black", fill="blue") +
  labs(title = "Bar Chart Example 2", x = "Air Quality Index/units",
       y = "Number/sites") +
  theme_minimal() +
  scale_x_discrete(labels = paste0
                   (breaks_list[-length(breaks_list)], "-", breaks_list[-1]))
            
@

\subsection{Heatmaps and Tree Maps}
\\  
\\In this chapter, we explore two powerful data visualisation techniques: heatmaps and treemaps. These methods are instrumental for conveying intricate data structures and patterns, offering unique ways to represent multivariate information, making them indispensable tools for data scientists.
\\  
\\We will delve into the theory behind heatmaps and treemaps, understand how to create them using popular data visualization libraries, and demonstrate their practical applications with real-world examples. By the end of this chapter, you will be well-equipped to leverage heatmaps and treemaps to gain insights from complex and hierarchical datasets.

\subsubsection{Heatmaps - Fire in Brazil}
\\  
\\The heatmap is a data visualisation technique that uses colour coding to represent different intensity.
\\  
\\In this illustrative example, heatmaps is used to visualize fire occurrences in Brazil. These heatmaps offer a spatially coherent representation, highlighting regions at high risk and seasonal patterns. Here, the heatmap is a power tool for identifying the risk of fire incidents. The data-driven insights empowers us to make informed decisions concerning preventive measures and strategies for firefighting.

<<load-data, echo=FALSE,message=FALSE,warning=FALSE>>=
# Data from NASA: https://firms.modaps.eosdis.nasa.gov/

brazil_fire_fy13 <- readr::read_csv('https://firms.modaps.eosdis.nasa.gov/data/country/modis/2013/modis_2013_Brazil.csv')
brazil_fire_fy14 <- readr::read_csv('https://firms.modaps.eosdis.nasa.gov/data/country/modis/2014/modis_2014_Brazil.csv')
brazil_fire_fy15 <- readr::read_csv('https://firms.modaps.eosdis.nasa.gov/data/country/modis/2015/modis_2015_Brazil.csv')
brazil_fire_fy16 <- readr::read_csv('https://firms.modaps.eosdis.nasa.gov/data/country/modis/2016/modis_2016_Brazil.csv')
brazil_fire_fy17 <- readr::read_csv('https://firms.modaps.eosdis.nasa.gov/data/country/modis/2017/modis_2017_Brazil.csv')
brazil_fire_fy18 <- readr::read_csv('https://firms.modaps.eosdis.nasa.gov/data/country/modis/2018/modis_2018_Brazil.csv')
brazil_fire_fy19 <- readr::read_csv('https://firms.modaps.eosdis.nasa.gov/data/country/modis/2019/modis_2019_Brazil.csv')
brazil_fire_fy20 <- readr::read_csv('https://firms.modaps.eosdis.nasa.gov/data/country/modis/2020/modis_2020_Brazil.csv')
brazil_fire_fy21 <- readr::read_csv('https://firms.modaps.eosdis.nasa.gov/data/country/modis/2021/modis_2021_Brazil.csv')
brazil_fire_fy22 <- readr::read_csv('https://firms.modaps.eosdis.nasa.gov/data/country/modis/2022/modis_2022_Brazil.csv')
@

<<data-cleaning, echo=FALSE,message=FALSE,warning=FALSE>>=
data_list <- list(
  brazil_fire_fy13,
  brazil_fire_fy14,
  brazil_fire_fy15,
  brazil_fire_fy16,
  brazil_fire_fy17,
  brazil_fire_fy18,
  brazil_fire_fy19,
  brazil_fire_fy20,
  brazil_fire_fy21,
  brazil_fire_fy22)

# Function to filter confident fire observation
filter_fire <- function(data) {
  filter_fire <- data %>% filter(confidence >= 95)
  return(filter_fire)
}

# Apply the filtering function to all data frames in the list
confident_fire_decade <- lapply(data_list, filter_fire)

# Access filtered data for a specific fiscal year, fy22
confident_fire_fy22 <- confident_fire_decade[[10]]
@

\\From the heatmap, we can observe that certain locations have significantly higher fires count. However, we do not know the cause of this. Is this due to geographical location, or is it because fires were mostly man-made and used to clear forest areas for agriculture use?
\\  
\\Colour selection from \href{https://colorbrewer2.org/#type=sequential&scheme=Oranges&n=9}{colorbrewer2}.

<<pivot-data-by-month, echo=FALSE,message=FALSE,warning=FALSE>>=
# Create a pivot table, no. of fire occurrences vs. Months (Jan-Dec), in FY22.
confident_fire_months_fy22 <- confident_fire_fy22 %>%
  mutate(acq_date = as.Date(acq_date, format = "%Y-%m-%d")) %>%
  group_by(month = floor_date(acq_date, 'month')) %>%
  summarize(count = n())

# change "2022-01-01" to Jan etc.
confident_fire_months_fy22$abb_month <- format(confident_fire_months_fy22$month, "%b")

# Create a custom order for the months
custom_order <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

# Reorder the "abb_month" factor based on the custom order
confident_fire_months_fy22$abb_month <- factor(confident_fire_months_fy22$abb_month, levels = custom_order)
@

<<spacetime-fy22,fig.height=7,fig.cap='Frequency of Fire by Space and Time, FY22',fig.show='hold',message=FALSE,warning=FALSE>>=
# Obtain the Brazil map data
brazil_map <- map_data("world", region = "Brazil")

# Create the heatmap of fire occurrences
space_heatmap <- ggplot(confident_fire_fy22, aes(x = longitude, y = latitude)) +
  geom_polygon(data = brazil_map, aes(x = long, y = lat, group = group), 
               fill = "#bdbdbd") +
  geom_bin2d(bins = 300) +
  scale_fill_gradient(low = "#fee6ce", high = "#d7301f") +
  coord_fixed(ratio = 1) +
  theme_minimal()+
  theme(axis.text = element_text(size = 10))

time_heatmap <- ggplot(confident_fire_months_fy22, 
                       aes(x = abb_month, y = as.character(2022), fill = count)) +
  geom_tile(width = 0.9, height = 1) +  # Create the heatmap tiles
  scale_fill_gradient(low = "#fff7ec", high = "#d7301f") +
  labs(x = "Month", y = "FY22", name = "count") +
  theme_minimal() +
  theme(axis.text = element_text(size = 10))

spacetime_fy22 <- grid.arrange(space_heatmap, time_heatmap, nrow = 2, heights = c(6,1.5))

print(spacetime_fy22)
@
\\From the table, we can clearly see that August and September are the riskiest months in terms of fire hazard, whereas November to July hardly pose any risk at all. It's natural to ask the follow-up question: How does FY22 compare to previous years? Is it valid to claim that August and September are the fire hazard season?

<<pivot-table-fy13-22, echo=FALSE,message=FALSE,warning=FALSE>>=
# Combine all the confident fire datasets into a single dataset
combined_confident_fire <- bind_rows(confident_fire_decade)

# Create a pivot table for fy13-22
pivot_table <- combined_confident_fire %>%
  mutate(acq_date = as.Date(acq_date, format = "%Y-%m-%d"),
         month_year = format(acq_date, "%b %Y")) %>%
  group_by(month_year) %>%
  summarize(count = n())

# Add column "abb_month" and "year" to the dataset "pivot_table"
pivot_table <- pivot_table %>%
  mutate(abb_month = gsub(".*?([A-Za-z]{3}).*", "\\1", month_year)) %>%
  mutate(abb_month = factor(abb_month, levels = custom_order)) %>%
  mutate(year = as.numeric(substring(month_year, nchar(month_year) - 3, nchar(month_year))))
@

<<fire-by-months-fy13-22,fig.height=3.5,fig.width=5.6,fig.cap='Frequency of Fire Occurrences, FY13-22',fig.show='hold'>>=
heatmap_plot <- ggplot(pivot_table, aes(x = factor(abb_month, levels = custom_order), 
                                        y = as.character(year), fill = count)) +
  geom_tile() +
  scale_fill_gradient(low = "#fff7ec", high = "#d7301f") +
  labs(x = "Month", y = "FY13-22") +
  theme_minimal() +
  theme(axis.text = element_text(size = 10))

print(heatmap_plot)
@
\\Indeed, the data showed a trend indicating that August to October have more fire occurrences compared to the rest of the year. There are clearly more fire hazards in those months.
\\  
\\The foundation of a heatmap is a data matrix, where each entry in this matrix represents an observation or measurement. Therefore, the first step to create a heatmap is to organize the data by columns and rows. In Figure~\ref{fig:fire-by-months-fy13-22}, the structured data is displayed as a grid of coloured cells, where the colour intensity corresponds to the underlying frequency.
\\  
\\Heatmaps are powerful tools for visualizing relationships between covariables within a model. One example of why we need to analyse a matrix of correlations between variables is in regression models. In the real world, variables are often correlated, and complete independent relations are rare. Consequently, analysing pairwise correlations is essential. Highly correlated variables significantly impact the regression model. When faced with highly correlated variables, we need to choose one variable from the correlated set. The selection is based on finding a regression model with the least Akaike Information Criterion (AIC) score among these variables. The AIC measures how much the linear model overfits the dataset. In other words, we want the regression model to explain the trend, and we do not want to overfit the model so that it explains the noise in the data set, which would lead to inaccurate predictions, as shown in Figure \ref{fig:noisy}.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{/Users/margaretli/Data-Visualisation-Project/overfitting.png}
    \caption{Danger of overfitting the regression model}
    \label{fig:noisy}
\end{figure}

\subsubsection{Treemaps}
Treemaps are a visualisation method specifically designed for hierarchical data structures. They represent data as nested rectangles, where each rectangle represents a part of the whole. Treemaps offer a visually appealing and efficient way to convey the hierarchical composition of data. The size and color of each rectangle can be used to encode additional information.

\subsubsection{Use Cases for Treemaps}
Treemaps are highly effective when dealing with hierarchical data. Some common use cases include:
\begin{itemize}
\item \textbf{Disk Space Visualization}: Treemaps can be employed to visualize disk space usage, where the outermost rectangle represents the entire disk, and inner rectangles represent folders and files. The size of each rectangle reflects the space they occupy.
\item \textbf{Market Share Analysis}: In business, treemaps are useful for visualizing market share data. The top-level rectangle represents the total market, and inner rectangles represent individual segments, brands, or products. The size and color of each segment can represent its share and performance.
\end{itemize}

XXX %missing Treemaps


\subsection{Line Charts and Time Series Visualization}

A \textbf{Line chart}, often referred to as a line graph or line plot, is a statistical chart composed of a Cartesian coordinate system, some points, and lines. It is commonly used to represent changes in numerical values over continuous time intervals or ordered categories. In a line graph, the x-axis is typically used for continuous time intervals or ordered categories (such as Stage 1, Stage 2, Stage 3). The y-axis is used for quantified data, and if it is negative, it is plotted below the y-axis. Lines are used to connect adjacent data points.

Line graphs are used to analyze trends in things that change over time or ordered categories. If there are multiple sets of data, they are used to analyze the interaction and impact of these data sets over time or ordered categories. The direction of the line represents positive/negative changes, and the slope of the line indicates the degree of change.

In terms of data, a line graph requires a continuous time field or a categorical field and at least one continuous data field.


\subsubsection{Basic Components}
\begin{itemize}
  \item \textbf{X-Axis (Horizontal Axis):} Typically represents the independent variable, such as time or date.
  \item \textbf{Y-Axis (Vertical Axis):} Typically represents the dependent variable, like sales numbers, stock prices, or temperatures.
  \item \textbf{Line:} Connects the individual data points. In some line charts, multiple lines can represent different categories or sets of data.
\end{itemize}


\subsubsection{Suitability for Displaying Trends Over Time:}
\begin{itemize}
    \item \textbf{Visual Clarity:} Line charts provide a clear and concise way to view changes over time. When data points are plotted over regular intervals (e.g., days, months, years), it becomes easy to see upward or downward trends.
    \item \textbf{Comparisons:} When you have multiple lines on a single chart, you can easily compare different sets of data. For instance, comparing sales data of two different products over time.
    \item \textbf{Identification of Patterns:} Line charts help in identifying patterns and anomalies. Seasonal patterns, cyclical events, and unexpected spikes or dips become evident.
    \item \textbf{Forecasting:} By viewing historical data trends on a line chart, analysts can make predictions or forecasts for future data points.
    \item \textbf{Simplicity:} They are easy to understand and interpret. Even if someone isn't data-savvy, they can grasp the general trend and major fluctuations from a line chart.
    \item \textbf{Flexibility:} They can be used for both short-term and long-term data. Whether you're looking at stock prices minute-by-minute over a single day or global temperature averages over a century, line charts can effectively represent the data.
\end{itemize}


\subsubsection{Limitations:}
While line charts are excellent for displaying trends over time, they have limitations. They may not be suitable for showing individual data distributions or for data where there's no logical order. eg. too many points, too many lines, too many zeros.\\


\subsubsection{Discuss the importance of time series visualisation in data analysis.}

Time series visualization refers to the graphical representation of time-ordered data points. In the world of data analysis, this form of visualization is invaluable for examining patterns, anomalies, and trends in datasets that evolve over time.\\
\textbf{Uncovering Trends:}\\
One of the primary advantages of time series visualization is the ease with which it allows analysts to identify long-term upward or downward trends in data. Recognizing these trends can help organizations make informed decisions about future strategies or interventions.\\
\textbf{Detection of Seasonality:}\\
Many datasets exhibit patterns that repeat over specific intervals, such as days, months, or years. Time series visualization makes it straightforward to spot such cyclical behaviors, which can be vital for businesses in sectors like retail or agriculture.\\
\textbf{Identifying Anomalies:}\\
Graphical representations can quickly highlight data points or periods that deviate significantly from the norm. These anomalies can indicate errors in data collection, or they may reveal significant events that need to be further investigated.\\
\textbf{Forecasting and Predictions:}\\
After identifying patterns in historical data, time series visualizations can aid in modeling future data points. Predictive modeling, underpinned by clear visualizations, allows businesses to make proactive decisions.\\
\textbf{Facilitating Comparative Analysis:}\\
Time series charts often allow for overlaying multiple data series on a single graph. This capability is useful for comparing different datasets or the same dataset under different conditions, leading to more comprehensive insights.\\
\textbf{Conclusion:}\\
Time series visualization is an indispensable tool in the arsenal of data analysts. It condenses large volumes of chronological data into easily interpretable graphics, enabling quick insights, better decision-making, and a deeper understanding of temporal dynamics in datasets. By providing a clear view of data trends, seasonality, and anomalies, time series visualization facilitates more informed and strategic actions in various domains.


\subsubsection{Provide best practices for creating clear and informative line charts.}

\begin{itemize}
  \item Title and Labels: Every chart should have a descriptive title and axis labels to clearly convey the purpose of the visualization and the data being shown.
  \item Use of Colors: Colors should be chosen to clearly differentiate between different lines or data points but also be consistent with the overall theme or style.
  \item Gridlines and Background: Soft gridlines can help the viewer estimate values. A clean background aids in clarity.
  \item Line Types and Point Shapes: When multiple lines are on the same chart, use different line types and point shapes to differentiate between them.
  \item Consistent Scaling: The scale on the y-axis should be consistent so that the viewer isn't misled.
  \item Annotations: Important points or changes can be annotated directly on the graph.
  \item Legends: If there are multiple lines or data points with different colors/shapes, a legend should be provided.
\end{itemize}

Let's apply these practices:


First, we generate 2 series of random data.

<<first-chunk>>=
x <- seq(1, 20)
y <- runif(20)
data <- data.frame(x = x, y = y)
@

Below is a line chart of the random sample:

<<plot-chunk, echo=FALSE, fig.height=5, fig.width=7>>=
ggplot(data, aes(x = x, y = y)) +
  geom_line(color = "blue") +
  labs(
    title = "Random Sample Line Chart",
    subtitle = "A demonstration of practice for line chart",
    x = "X-axis Label",
    y = "Y-axis Label",
    caption = "Source: Randomly generated data"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.title.x = element_text(margin = margin(t = 10)),
    axis.title.y = element_text(margin = margin(r = 10))
  ) +
  scale_x_continuous(breaks = seq(1, 20, by = 1))
@


\subsubsection{Showcase real-world examples of time series visualisations}

Time series of the daily CNY, CAN, EUR, HKD, USD versus GBP exchange reference rate data
published by the European Central Bank over the time period from 01 Jan 2013 to 12 Oct 2023 (without weekends). The exchange rate tells you how many pounds you need to buy/sell 1 CNY, CAN, EUR, HKD, USD.

\subsubsection{The data set has the format as below:}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Date} & \textbf{CNYtoGBP} & \textbf{CANtoGBP} & \textbf{EURtoGBP} & \textbf{HKDtoGBP} & \textbf{USDtoGBP} \\
\hline
\%d-\%m-\%y & Value & Value & Value & Value & Value \\
\hline
& & & & & \\
\hline
\end{tabular}
\caption{Field Information: CNY, CAN, EUR, HKD, USD to GBP}
\end{table}


\subsubsection{Multiple time series in one plot:}

<<echo=FALSE>>=
#Read in the data:
MyData <- read.csv("exchangeRate.csv",
                   header = TRUE, sep = ",",
                   dec = ".",
                   fileEncoding="UTF-8-BOM")

MyData <- data.frame(MyData)

# Convert Date to a Date object
MyData$Date <- as.Date(MyData$Date, format="%d-%b-%y")

# Order the data by Date
MyData <- MyData[order(MyData$Date), ]

# Convert columns to time series
ts_data <- lapply(MyData[-1], ts,
                  start=c(as.numeric(format(min(MyData$Date), "%Y")),
                          as.numeric(format(min(MyData$Date), "%j"))),
                  frequency=365)

# Plot time series
plot(MyData$Date, ts_data$CNYtoGBP, type="l",
     col="blue", ylim=range(MyData[-1]),
     ylab="Exchange Rate", xlab="Date",
     main="Exchange Rates Over Time")

lines(MyData$Date, ts_data$CANtoGBP, col="red")
lines(MyData$Date, ts_data$EURtoGBP, col="green")
lines(MyData$Date, ts_data$HKDtoGBP, col="purple")
lines(MyData$Date, ts_data$USDtoGBP, col="brown")
legend("topright", legend=names(ts_data),
       fill=c("blue", "red", "green", "purple", "brown"))
@


<<>>=
# Calculate 21-day moving average for each currency
columns <- names(MyData)[!names(MyData) %in% "Date"]
for (col in columns) {
  new_col_name <- paste0(col, "_MA7")
  MyData[[new_col_name]] <- zoo::rollapply(MyData[[col]], width=21, FUN=mean, fill=NA, align='right')
}

# Plotting
plot_data <- MyData %>% gather(key="Currency", value="Rate", -Date) %>%
  filter(grepl("MA7", Currency))

ggplot(plot_data, aes(x=Date, y=Rate, color=Currency)) +
  geom_line() +
  labs(title="7-Day Moving Average of Exchange Rates",
       subtitle="",
       y="Exchange Rate to GBP (21-Day MA)",
       x="Date",
       color="Currency") +
  theme_minimal() +
  scale_color_brewer(palette="Set1")
@


\subsubsection{Decomposition of one time series into trend, seasonal, and random.}


One of the primary advantages of time series visualization is the ease with which it allows analysts to identify long-term upward or downward trends in data and patterns that repeat over specific intervals. By decomposing the time series, it would be easy to see those features.

<<echo=FALSE>>=
plot(decompose(ts_data$CNYtoGBP), xlab="Date")
@



\subsubsection{Double y-axis time series plot.}

If we want to display two different time series that measure two different quantities at the same time points, we can draw the second series again on the second Y-axis on the right side.

<<echo=FALSE>>=
# Plot the first time series with its y-axis
plot(ts_data$CNYtoGBP, type="l", col="blue", ylab="Exchange Rate", xlab="Date",
     main="Exchange Rates Over Time")

# Add the second time series with a secondary y-axis
par(new=TRUE)
plot(ts_data$EURtoGBP, type="l", col="red", axes=FALSE, xlab=NA, ylab=NA)
axis(side=4)  # Add the secondary y-axis on the right
mtext("Temp", side=4, line=3)  # Label the secondary y-axis

# Add a legend to distinguish the two time series
legend("topright", legend=c("CNYtoGBP", "EURtoGBP"),
       col=c("blue", "red"), lty=1, cex=0.8)
@


\subsection{Network Graphs}
\textbf{Definition and Utility:}
Network graphs, often referred to as graphs or networks, are a powerful data visualization method used to depict relationships between entities. These entities, known as nodes, are interconnected by edges or links, which represent relationships, connections, or interactions. Network graphs find extensive utility in various fields, such as social network analysis, transportation systems, and even biological networks like protein-protein interactions. They excel at revealing complex dependencies and structures, making them a critical tool for understanding relational data.

\subsubsection{The Mathematics behind Network Graphs:}
Constructing network graphs involves several mathematical intricacies. Here we present just a few of the many concepts that play a role in the creation of such graphs:
\begin{enumerate}
\item \textbf{Nodes and Edges}: Mathematically, a network graph, \(G\), is defined as \(G = (V, E)\), where \(V\) represents the set of nodes and \(E\) represents the set of edges connecting these nodes.
\item \textbf{Node Degree}: The degree of a node is the number of edges connected to it. In a directed graph, nodes can have both in-degrees and out-degrees.
\item \textbf{Centrality Measures}: Centrality metrics like degree centrality, betweenness centrality, and closeness centrality provide insights into the relative importance or influence of nodes within a network.
\item \textbf{Graph Metrics}: Graph theory concepts like shortest paths, connected components, and clustering coefficients are used to analyze the network's structure.
\end{enumerate}

\textbf{Formulas used in Network Graphs:}

\begin{enumerate}
\item \textbf{Degree of a Node (Undirected Graph)}:
\[
\text{Degree}(v) = \sum_{w \in V} A(v, w)
\]
where \(A(v, w)\) is the adjacency matrix element, indicating whether there is a connection between nodes \(v\) and \(w\).
\item \textbf{Degree of a Node (Directed Graph)}:
\[
\text{In-Degree}(v) = \sum_{w \in V} A(w, v)
\]
\[
\text{Out-Degree}(v) = \sum_{w \in V} A(v, w)
\]
\item \textbf{Betweenness Centrality (for unweighted graphs)}:
\[
C_B(v) = \sum_{s \neq v \neq t} \frac{\sigma_{st}(v)}{\sigma_{st}}
\]
where \(\sigma_{st}\) is the number of shortest paths from node \(s\) to \(t\), and \(\sigma_{st}(v)\) is the number of those paths passing through node \(v\).
\end{enumerate}

\subsubsection{Network Graphs in Practice}


<<Network-setup-chunk, echo=FALSE>>=
# Load the Les Miserables dataset
#data("lesmis")

# Create a graph from the Les Miserables dataset
#lesmis_graph <- graph_from_data_frame(lesmis$edges, directed = FALSE)

# Customize the graph appearance
#V(lesmis_graph)$color <- "lightblue"
#V(lesmis_graph)$size <- 10
#V(lesmis_graph)$label <- V(lesmis_graph)$name

# Set the layout
#layout <- layout_with_fr(lesmis_graph)
@

\begin{figure}[h]
\centering
<<network-plot, echo=TRUE, fig.height=5, fig.width=7, fig.caption='Character Interactions in Les Misérables'>>=
# Plot the graph
#plot(lesmis_graph, layout = layout, vertex.label.cex = 0.7, main = "Character Interactions in Les Misérables")

@
\end{figure}

\subsection{Sankey Diagrams}
xxx

\subsection{Geographic Maps and Spatial Data Visualisation}
xxx

\subsection{3D and Interactive Visualisations}
xxx

\subsection{Advanced Visualisation Techniques}
xxx




\section{Practical Implementations}
XXX




\section{Case Studies}
\subsection{Market Analysis Dashboards}
XXX
\subsection{Healthcare Data Visualisation}
XXX




\section{State-of-the-Art Approaches}
XXX



\section{Conclusion}
XXX


\end{document}
