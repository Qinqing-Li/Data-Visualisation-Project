\documentclass{article}
\usepackage{hyperref}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{appendix}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\numberwithin{equation}{section}
\usepackage{comment}
\usepackage{natbib}
\usepackage[a4paper, total={6in, 8in}]{geometry}

\title{Data Visualisation: Theory and Practice}
\author{Yujie Chu, Pia Fullaondo, Qinqing Li, Jacko Zhou}

\begin{document}

<<setup, echo=FALSE, message=FALSE, warning=FALSE, include = FALSE>>=
library(knitr)
library(maps)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(lubridate)
library(readxl)
library(forecast)
library(plotly)
library(sf)
library(tmap)
library(gridExtra)
library(igraph)
library(scatterplot3d)
library(stats)
library(gridExtra)
library(cowplot)
library(datasets)
library(zoo)
library(viridis)
library(graphics)
library(palmerpenguins)
library(Rtsne)
library(pROC)
data(ToothGrowth)
data(mtcars)
data(trees)
opts_chunk$set(fig.path = 'figure/beamer-',
               fig.align = 'center',
               fig.show = 'hold', size = "scriptsize",
               fig.pos = 'H')

@

\maketitle 

\newpage
\tableofcontents

\newpage  % Start overview section on a new page
\section*{Overview}
\addcontentsline{toc}{section}{Overview} % Add section to the table of contents

\noindent
Graphical statistics is an indispensable tool in the arsenal of every contemporary data scientist. Primarily, it enables users to delve into data, explore its nuances, and effectively communicate empirical discoveries through visual means. This project aims to provide an overview of modern data visualisation methods, delve into their theoretical underpinnings, and demonstrate the potency of these methodologies using both synthetic and authentic datasets. Computational implementations via dashboards — using Python Dash or R Shiny — are used to showcase the methodologies in practice. Furthermore, the project is aware of the ongoing evolution of novel data visualisation techniques, recognising it as an active area of research (e.g. Rodu and Kafadar, 2022), and thus aims to cover both textbook methods as well as state-of-the-art approaches and open problems.\\

\noindent
This thesis aims to explore the theoretical foundations that underlie various visualisation methodologies, study their mathematical construction, and illustrate their practical application. In particular, Chapters 3, 4, and 5 concentrate on standard implementations of Graphical Statistics for univariate, bivariate, and multivariate datasets respectively. Chapter 3 discusses the Kernel Density Estimate, ROC Curve, and Time Series Analysis, showcasing their use through the Tooth Growth and the Exchange Rate datasets. Chapter 4 is focused on the topic of regression, discussing the theory behind Simple Linear Regression followed by LOESS regression. Chapter 5 introduces the model selection method AIC and Multiple Linear Regression, followed by the theoretical foundation of biplots and t-SNE. Subsequent chapters cover active research areas in Graphical Statistics and implementation of dashboards. \\


\noindent
Throughout this thesis, R package names such as \textit{ggplot2} are italicised, while R commands like \texttt{faceting} are displayed in a teletype font.The R code chunks for each figure in the report can be found in a separate \href{https://github.com/Qinqing-Li/Data-Visualisation-Project/tree/main/code}{GitHub code folder}. \\

\noindent \textbf{Literature and Research}\\ %This section doesn´t seem to be too relevant here (the overview of our work)
\noindent
Data visualisation is an area of interest in numerous method-specific publications. One notable publication is the \href{https://www.tandfonline.com/journals/ucgs20}{Journal of Computational and Graphical Statistics}. This journal publishes ongoing research on the latest techniques in computational and graphical methods in statistics, encompassing data analysis and numerical graphical displays. \href{https://www.sciencedirect.com/journal/spatial-statistics}{Spatial Statistics} publishes articles on the theory and application of spatial and spatio-temporal statistics. \href{https://journal.r-project.org}{The R Journal} publishes research articles in statistical computing that are of interest to R users. These together reveal the vast potential for current and future research in this ever-evolving domain, particularly as our digital landscape and data sphere expand at an accelerated pace.\\

\newpage 

\section{Introduction}

\noindent
This chapter provides case examples demonstrating both the influential applications and potential misuses of data visualisations, with the aim of underscoring the enduring importance of data visualisation.These are sourced from BBC Ideas \cite{bbcdatavis}. Following this, an introduction to the R package \textit{ggplot2} is provided, along with a presentation of the datasets used in this thesis.

\subsection{Historical Background and Misuses of Data Visualisation}
 
\\\textbf{Motivations for Using Data Visualisations — Case 1}

\noindent
Florence Nightingale was not only a social reformer and the founder of modern nursing but also a pioneering statistician. It was her application of data visualisation during the Crimean War that transformed the field of healthcare and pushed for social reform.\\  

\noindent
During the Crimean War, Nightingale recognised that unsanitary hospital conditions were claiming more lives than the battlefield itself. With the help of William Farr, Nightingale created the coxcomb aimed to illustrate the toll of preventable mortality on soldiers, as shown in Figure~\ref{fig:coxcomb}. The coxcomb, resembling an unconventional pie chart, partitioned mortality by causes. Blue indicates preventable deaths, red indicates deaths by wounds, and black indicates other causes. The blue areas outweighed the red and black sections combined, highlighting the disproportionate impact of unsanitary hospital conditions on the mortality rate.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{image_reference/Nightingale-mortality.jpg}
    \caption{``Diagram of the causes of mortality in the army in the East" (1858) by Florence Nightingale \cite{graphFN}}
    \label{fig:coxcomb}
\end{figure}

\noindent
Nightingale leveraged the compelling visualisations in her advocacy efforts, presenting them to MPs and government officials who otherwise are unlikely to read or understand statistical reports. Nightingale successfully persuaded Queen Victoria, head of the British Army at the time, to allocate funding for the improvement of better conditions in military hospitals.\\

\noindent
\textbf{Motivations for Using Data Visualisations — Case 2}
\\
\noindent
Sometimes, one glance is enough to convey the most powerful idea. Edward Hawkins, a British climate scientist and Professor of climate science at the University of Reading, is renowned for his exceptional datavisualisations of climate change.\\

\noindent
In 2018, Edward Hawkins was invited to deliver a lecture on climate change in Wales to an audience with diverse backgrounds. It was important to effectively convey the growing urgency surrounding global warming. To achieve this, he created a chart that used just colours, without any words, titles, or legends, as shown in Figure~\ref{fig:global}. This seemingly simple yet remarkably powerful chart visually illustrated the Earth's warming trend since 1850.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{image_reference/global.png}
    \caption{Latest global stripes (1850-2020) by Edward Hawkins \cite{blog}}
    \label{fig:global}
\end{figure}

\noindent 
Known as the ``warming stripes," this chart cleverly employs blues to indicate cooler-than-average years and reds to signify hotter-than-average years. Its influence reached far and wide, gracing the front pages of major media outlets and featured in news broadcasts worldwide. It became a symbol in climate change demonstrations. Arguably, it stands as one of the most iconic graphics in modern times.\\

\noindent
\textbf{Misuses of Data Visualisation — Case 1}

\noindent
Having observed the remarkable effectiveness of data visualisations, the significance of employing them correctly becomes apparent. The improper use of data visualisations holds the potential to significantly influence the public in misleading ways, resulting in undesired consequences.\\

\noindent
Inappropriate data visualisation can conceal trends rather than reveal them. Figure~\ref{fig:misuse1} illustrates an instance of this issue. On the left-hand side, an inappropriate scale is used - the y-scale ranging from 0 to 30 million dollars, obscuring the fluctuations in payroll spending. Conversely, on the right-hand side, observe that there's a significant increase of over 500,000 dollars in just two months. This revelation is substantial; considering inflation, 500,000 dollars in 1937 is worth well over 10 million dollars today \cite{worth}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{image_reference/misuse1.png}
    \caption{Incomplete Data Analysis, ``How to lie with statistics" by Miguel de Carvalho \cite{lie}}
    \label{fig:misuse1}
\end{figure}

\noindent 
\textbf{Misuses of Data Visualisation — Case 2}

\noindent
One striking example of data visualisation misuse is found in the Kallikak Family tree - one of the most prominent eugenic narratives of the 20th century.\\

\noindent
The visualisation (as shown in Figure~\ref{fig:familytree}) was created by the psychologist Henry Goddard and presented in his 1912 book, ``The Kallikak Family: A Study in the Heredity of Feeble-Mindedness." Goddard's narrative centered around Martin Kallikak, a soldier who, in addition to his marriage to a respected citizen, had a one-night stand with a ``feeble-minded" maid. Goddard believed that intellectual disabilities were inherited traits. In Goddard's account, the legitimate family was successful, while the children of the ``feeble-minded" maid were labeled as ``the lowest types of human beings." However, research has since revealed that the entire story was fictitious, as there was no record of the maid's existence \cite{fakedata}.\\

\noindent
Regrettably, the Kallikak family tree became a central element in the eugenics movement. Figure \ref{fig:familytree} was featured in the 1935 Nazi propaganda film ``Das Erbe" (The Inheritance), which was used to promote public acceptance of Nazi eugenics laws. This propaganda laid the groundwork for the forced sterilization of approximately 400,000 people under Nazi eugenics policies \cite{eugenics}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{image_reference/familytree.jpg}
    \caption{The Kallikak Family tree (1912) by Henry Goddard \cite{ktree}}
    \label{fig:familytree}
\end{figure}

\subsection{Computing and Data Visualisation}

As a powerful tool for statistical analysis and graphic creation, R offers a wide array of functionalities to cater to a range of data visualisation needs. In data visualisation, \textit{ggplot2} is the main tool used to create plots capable of representing various types of datasets. The foundation of \textit{ggplot2} lies in the Grammar of Graphics, allowing the sequential construction of individual elements composing a graph, which are subsequently combined to create a unified graphical representation.\\

\noindent\textbf{Key R Commands}\\
\noindent 
As shown in subsequent sections, it is possible to produce visualisations rather effortlessly by utilising internal data from R, or even external data, together with \textit{ggplot2}. When using \textit{ggplot2}, certain key tools are used repeatedly to create visual plots from datasets. Some of these include: \texttt{geom}, \texttt{scale}, \texttt{coord}, \texttt{theme}, and more.\\


\noindent 
These particular tools serve the following functions \cite{ggplot2}: %This section needs to be revised
\noindent
\texttt{geom} refers to geometric objects, pivotal components within \textit{ggplot2} used to define the visual representation of data in a plot. Each \texttt{geom} function corresponds to a specific graphical representation type in a chart. Additionally, in \textit{ggplot2}, \texttt{scales} functions enable the adjustment of various mapping details, including colour choices, label formatting, legend arrangement, and more.\\

\noindent \texttt{coord} provides the axes and gridlines which aid in interpreting the graph. Various coordinate systems such as cartesian, polar, and map projections are available. Furthermore, \texttt{faceting} is a robust feature enabling the partitioning of a single plot into multiple plots based on factors present in the dataset. This functionality proves particularly beneficial for exploring and presenting data with multiple groups or categories.\\

\noindent The \texttt{theme} function holds significance in customising the non-data elements of plots. The theme system within \textit{ggplot2} allows precise adjustment of aesthetic aspects such as fonts, labels, legends, and background colours. This tool is essential for enhancing plot readability and creating visually captivating graphics tailored to specific audiences or publication requirements.\\

\noindent\textbf{Key R Packages}\\
\noindent
The R language is a powerful data analysis tool, and its power lies in the wide variety of R packages that help it achieve a wide variety of functions. Through out this project, different packages, such as $maps$, $dplyr$, $tidyverse$, and $gridExtra$ will be used. These packages significantly enhances the analysis and presentation of data. \\

\noindent
The \textit{dplyr} packages plays a crucial role in data manipulation and transformation. It helps us filter, sort, summarise, and transform datasets, making data analysis more efficient and manageable. The \textit{maps} package is used for providing map data and tools, allowing one to easily draw and display maps in geospatial analyses. The \textit{tidyverse} package is indeed a collection of R packages designed to provide a cohesive and comprehensive set of tools for data science. It includes several widely used packages such as \textit{ggplot2} and \textit{dplyr}. The \textit{gridExtra} packages offers the capability to combine several graphical objects into a cohesive display.\\


\subsection{Datasets}
\noindent
This section introduces the datasets analysed and visualised in the subsequent chapters.\\

\noindent
\textbf{mtcars}: The built-in R dataset, \href{https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/mtcars}{mtcars} was extracted from the 1974 Motor Trend US magazine. It includes the fuel consumption and 10 aspects of automobile design for 32 automobiles (1973–74 models). \\

\noindent
\textbf{ToothGrowth}: The built-in R dataset, \href{https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/ToothGrowth}{ToothGrowth}, measures the tooth growth of 60 guinea pigs. Each animal was either administered with vitamin C or orange juice.\\

\noindent
\textbf{iris}: The built-in R dataset \href{https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/iris}{iris} was collected by Edgar Anderson. This dataset was famously used by British statistician Ronald Fisher to demonstrate linear discriminant analysis in 1936. It encompasses 5 flower characteristics for 3 types of iris, each with 50 samples. \\

\noindent
\textbf{Fire in Brazil}: Open-source fire observation data is provided by \href{https://firms.modaps.eosdis.nasa.gov/}{NASA}. The analysis focuses on Brazil (2013-2022). Note that the dataset contains the variable ``confidence", ranging from 0\% to 100\%. The dataset was filtered with a confidence level of $\ge$ 95\%  to ensure an accurate account of fire occurrences \cite{nasa_confidence}.\\

\noindent
\textbf{Exchange Rate}: The exchange rate data, available at the \href{https://www.bankofengland.co.uk/boeapps/database/index.asp?first=yes&SectionRequired=I&HideNums=-1&ExtraInfo=true&Travel=NIx}{Bank of England}, provides daily spot exchange rates against the pound Sterling (2005-present). This report examines the daily spot rates of the Canadian Dollar, Euro, and US Dollar against the pound Sterling.\\

\noindent
\textbf{Taipei Housing}: The historical real estate valuation from Sindian Dist., New Taipei City, Taiwan,  available at \href{https://archive.ics.uci.edu/dataset/477/real+estate+valuation+data+set}{UC Irvine Machine Learning Repository}.\\

\newpage 

\section{Theoretical Foundations of Data Visualisation}
This chapter, ``Theoretical Foundations of Data Visualisation," delves into the core principles and concepts that serve as the base of this field. We seek to understand not only the ``how" but also the ``why" behind the creation of visualisations that captivate and inform.

\subsection{Introduction to Data Visualisation Theory}
Creating effective data visualisations requires a robust theoretical framework underlying every chart, graph, or plot. These theoretical underpinnings not only form the basis of data visualisation but also influence how we represent, perceive, understand, and interpret data.\\ 

\noindent \textbf{Guiding Principles for Data Representation}\\
The theoretical framework of data visualisation involves guiding principles dictating visual representation of data. These principles include \textbf{accuracy}, emphasising faithful reflection of underlying data to reduce distortion or misinterpretation; \textbf{simplicity}, advocating for streamlined visuals to convey information effectively; \textbf{clarity}, ensuring visuals are easily understood without unnecessary complexity; \textbf{relevance}, presenting information pertinent to the message or question addressed; and \textbf{consistency}, maintaining uniform use of visual elements like color coding and labeling throughout a visualisation.\\

\noindent \textbf{Theoretical Framework and Visual Perception}\\
Understanding how the human brain processes visual information is a fundamental aspect of data visualisation theory. This knowledge plays a crucial role in designing visualisations that effectively connect with viewers. It encompasses several key considerations which will be studied in order: the Gestalt Principles, which encompass proximity, similarity, and continuity, affecting how visual elements are grouped and interpreted; Color Theory, involving the strategic use of color contrasts and harmonies to improve clarity and impact; and the management of Cognitive Load, which emphasises the importance of reducing mental effort needed to process information.

\subsection{Data Types and Visualisation Techniques}
To have a discussion about data representation, understanding the nature of the data is key. Data comes in various types, and selecting the appropriate visualisation technique is contingent upon recognising these distinctions. In this section, the data types are categorised and matched with their suitable visualisation techniques.

\subsubsection{Categorisation of Data Types}
Data types can be broadly categorised into four main types: 
\begin{itemize}
    \item \textbf{Nominal data}: represents categories or labels without any inherent order. Examples include colours, gender categories, and city names. 
    \item \textbf{Ordinal data}: implies a meaningful order or ranking among categories but lacks equal intervals between them. Examples include survey responses (eg. “very satisfied”, “satisfied”, “neutral”, “dissatisfied”, “very dissatisfied”)
    \item \textbf{Interval data}: possesses ordered categories with equal intervals between them, but it lacks a true zero point. Temperature is measured in Celsius or Fahrenheit as an example. 
    \item \textbf{Ratio data}: includes ordered categories with equal intervals and a meaningful zero point. Examples are age, income, and weight. 
\end{itemize}

\noindent \textbf{Matching Data Types with Appropriate Visualisation Techniques}\\
\noindent Selecting appropriate visualisation techniques is essential for effective data communication. Various data types demand specific visualisation methods for optimal representation \cite{healy2018data}. For \textbf{nominal data}, bar charts and stacked bar charts are effective in displaying categorical information and relative proportions. \textbf{ordinal data} benefits from ordered bar charts, dot plots, or stacked bar charts, maintaining the ranking and order of categories. \textbf{interval data} is best visualised using line charts, histograms, and box plots, showcasing trends and distributions without assuming a true zero point. \textbf{ratio data} finds effective representation through scatter plots, histograms, and line charts, enabling precise comparisons and measurements due to the presence of a meaningful zero point. These are represented in Figure~\ref{fig:data-plots}, where artificial data from a class of 4th-year university students is visualised.

<<data-plots, echo=FALSE, message=FALSE, fig.height=8, fig.width=11, fig.cap='Different types of visualisation techniques according to the data type'>>=

# Define the colors
my_palette <- c("#4B0082", "#459395", "#EB7C69", "#FDA638")

# Generate nominal data (gender categories and tendencies)
nominal_data <- data.frame(
  gender = c("Male", "Female", "Non-binary", "Prefer not to say"),
  tendency = c(30, 40, 15, 15)
)

# Stacked bar chart showing proportions for nominal data
plot1 <- ggplot(nominal_data, aes(fill = gender, y = tendency, x = 1)) +
  geom_bar(stat = "identity") +
  labs(title = "Gender Categories", x = NULL, y = "Percentage") +
  scale_fill_manual(values = my_palette) +
  theme(axis.text.x = element_blank(), 
        axis.ticks = element_blank(), # Remove axis ticks
        panel.grid.major = element_line(color = "lightgrey"),
        panel.background = element_rect(fill = "white"))   

# Generate ordinary data (student satisfaction)
ordinal_data <- data.frame(
  satisfaction_level = c("Satisfied", "Neutral", "Dissatisfied"),
  count = c(45, 30, 25)
)

# Ordered bar graph for customer satisfaction data
plot2 <- ggplot(ordinal_data, aes(x = reorder(satisfaction_level, -count), y = count)) +
  geom_bar(stat = "identity", fill = my_palette[2]) +
  labs(title = "Students Satisfaction Levels with the Course", x = NULL, y = "Count") +
  theme_minimal()

# Generating sample data for student attendance (replace this with your actual attendance data)
attendance_data <- data.frame(
  week = 1:11,  # Weeks of the semester
  attendance = c(20, 22, 25, 27, 30, 32, 28, 26, 25, 28, 30)  
)

# Line chart for temperature data
plot3 <-  ggplot(attendance_data, aes(x = week, y = attendance)) +
  geom_line(linewidth = 0.7, color = my_palette[3]) +
  labs(title = "Student Lecture Attendance", x = "Week of the semester", y = "Number of Students") +
  theme_minimal() +
  scale_y_continuous(breaks = seq(20, 32, by = 2), labels = function(x) round(x))  # Adjust the breaks and labels


# Generate data for income distribution across a population
income_data <- data.frame(
  income = c(40000, 45000, 45000, 50000, 50000, 50000, 55000, 60000, 60000, 40000, 45000, 45000, 50000, 50000, 50000, 55000, 60000, 60000, 40000, 45000, 45000, 50000, 50000, 50000, 55000, 60000, 60000)
)

# Create a histogram for income distribution
plot4 <-ggplot(income_data, aes(x = income)) +
  geom_histogram(binwidth = 5000, fill = my_palette[4], color = my_palette[4], alpha = 0.5) +
  labs(title = "Expected Income Distribution for Top Students", x = "Income (£/month)", y = "Frequency") +
  theme_minimal()

# Arrange plots in a 2x2 grid
grid.arrange(plot1, plot2, plot3, plot4, ncol = 2)
@
\subsection{Data Abstraction and Representation}
The transformation of raw data into meaningful representations is a pivotal step in data representation. This process, known as data abstraction, involves distilling complex datasets into visual forms that convey insights. In this section, we explore data abstraction, the hierarchies and levels of abstraction in data visualisation, and the critical trade-offs between abstraction and the potential loss of information.\\

\noindent \textbf{Data Abstraction: Transforming Raw Data}\\
Data abstraction involves simplifying and structuring raw data into comprehensible and insightful formats. This process serves as the bridge, transforming numbers, text, and variables into visual elements that convey patterns, trends, and relationships, forming the core of informative data visualisations \cite{tufte2001visual}.

\subsubsection{Hierarchies and Levels of Abstraction}
Abstraction operates on multiple levels of granularity. Hierarchies of abstraction allow us to represent data at varying levels of detail: 
\begin{enumerate}
    \item \textbf{Low-Level Abstraction}: At the lowest level, raw data is preserved in its most detailed form. This might include individual data points, measurements, or unprocessed text.
    \item \textbf{Mid-Level Abstraction}: At the mid-level, data is grouped or aggregated to provide a broader overview. For example, hourly data points may be aggregated into daily or weekly averages.
    \item \textbf{High-Level Abstraction}: At the highest level, data is represented in a condensed and abstracted form, often as summary statistics or key insights. This level provides a big-picture view.
\end{enumerate}

\noindent 
These different levels of abstraction are represented in Figure~\ref{fig:abs-plots}. The first visualisations of the mtcars dataset is a scatter plot that provides detailed information about the relationship between car weight and miles per gallon, with points colored by the number of cylinders. The second is an abstract visualisation using a box-and-whisker plot to provide a high-level summary of the distribution of miles per gallon for different numbers of cylinders. Finally, the third visualisation is a bar plot presenting aggregated information about the average miles per gallon for different numbers of cylinders.

<<abs-plots, echo=FALSE, fig.height=4, fig.width=12, fig.cap='Mtcars dataset visualised on 3 different levels of abstraction'>>=

# Define a dark red and blue color palette
dark_red_blue_palette <- c("4" = "#459395", "6" = "#EB7C69", "8" = "#FDA638")  # Adjust the shades of red and blue %jhgjgj

# Level 1: Detailed Scatter Plot
plot1 <- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +
  geom_point(size = 3, alpha = 0.8) +
  labs(
    title = "Low-Level Abstraction",
    x = "Weight (1000 lbs)",
    y = "Miles per Gallon",
    color = "Cylinders"
  ) +
  scale_color_manual(values = dark_red_blue_palette) +  # Apply the custom color palette
  theme_minimal()

# Level 2: Aggregated Bar Plot
summary_data <- aggregate(mpg ~ cyl, data = mtcars, FUN = mean)

plot2 <- ggplot(summary_data, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +
  geom_bar(stat = "identity", width = 0.7) +
  labs(
    title = "Mid-Level Abstraction",
    x = "Cylinders",
    y = "Average Miles per Gallon",
    fill = "Cylinders"
  ) +
  scale_fill_manual(values = dark_red_blue_palette) +  # Apply the custom color palette
  theme_minimal()

# Level 3: Abstract Box-and-Whisker Plot
plot3 <- ggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +
  geom_boxplot() +
  labs(
    title = "High-Level Abstraction",
    x = "Cylinders",
    y = "Miles per Gallon",
    fill = "Cylinders"
  ) +
  scale_fill_manual(values = dark_red_blue_palette) +  # Apply the custom color palette
  theme_minimal()

# Arrange plots in a grid
grid.arrange(plot1, plot2, plot3, ncol = 3)
@
\\
\\
\noindent \textbf{Trade-offs Between Abstraction and Information Loss}\\
While abstraction simplifies complex data, it presents trade-offs. Designers of data visualisation must strike a balance between clarity and detail, generalisation and specificity, and context versus precision. Abstraction increases clarity but may sacrifice crucial detailed information necessary for some analytical tasks. It offers a more generalised view accessible to a wider audience but might overlook specific nuances essential for experts. While providing valuable context, high-level abstraction may lack the precision required for precise decision-making.\\ 

\noindent 
In data visualisation, the art of data abstraction lies in finding the right level of detail that effectively conveys the intended message while minimising the risk of information loss. This balancing act is a critical consideration in the design of informative and meaningful data visualisations.

\subsection{Visual Perception and Cognition}
In this first section, human visual perception is explored, along with the application of cognitive psychology principles in data visualisation.\\ 

\noindent \textbf{Human Visual Perception: Decoding Visual Information}\\
Human visual perception profoundly influences our understanding of the surrounding world. When applied to data visualisation, it shapes the way that individuals engage with and derive meaning from visual data representations.\\

\noindent 
Significant aspects of human visual perception within data visualisation encompass \textbf{pattern recognition}, adept at identifying trends, outliers, and relationships in data representations. Additionally, \textbf{perceptual grouping}, where visually similar elements are grouped together, influences the interpretation of data clusters and shapes. Moreover, the \textbf{hierarchy of perception} dictates that certain visual attributes are processed more swiftly and effectively than others, such as color being processed faster than text, influencing the viewer's attention hierarchy \cite{dastani2002role}.\\

\noindent \textbf{The Gestalt Principles \index{The Gestalt Principles}}\\
Furthermore, the Gestalt Principles play an important role in the realm of visual perception and design \cite{rosli2015gestalt}. Key Gestalt principles crucial in shaping visual information perception include proximity, which groups related elements, \textbf{similarity} that links similar attributes, \textbf{continuity} aiding trend representation, \textbf{closure} for implying connections, and \textbf{symmetry} for balance and aesthetics in visualisations \cite{todorovic2008gestalt}. \\

\noindent By harnessing the principles of human visual perception, applying insights from cognitive psychology, and leveraging pre-attentive attributes, data visualisation designers can create visualisations that are not only aesthetically pleasing but also cognitively efficient.	


\subsection{Colour Theory in Data Visualisation}
In this section, the significance of colour in data visualisation, the principles of colour perception and encoding, and the importance of avoiding misleading visualisations through thoughtful colour choices are explored.\\

\noindent \textbf{The Importance of Colour in Conveying Information}\\
Colour significantly enhances the impact and comprehension of data visualisations. It serves multiple purposes: distinguishing data points, emphasising trends, and offering contextual information. It is utilised to encode categorical data, differentiating between various groups with distinct colours, and to represent quantitative data by using colour intensity or gradients to portray values or magnitudes. Additionally, color is instrumental in adding context to visualisations through background elements, labels, or annotations, imparting meaning to the data \cite{healy2018data}.\\

\noindent \textbf{Colour Perception and Colour Encoding in Visualisations}\\
Understanding color perception in data visualisation is crucial. Key principles involve considering color discrimination, ensuring accessibility for individuals with color vision deficiencies, as is illustrated by Figure~\ref{fig:colour-plot}. Careful selection of color schemes aligned with the intended message is essential — for instance, using warm colours like red and orange to indicate caution or warmth, and cool colours like blue and green to convey calmness or coldness. Additionally, attention should be paid to how colours interact when combined; certain combinations might create visual vibrations or impact text legibility.\\

<<colour-plot, echo=FALSE, fig.height=3, fig.width=10, fig.cap='Colour perception of a heatmap for by a colour blind person'>>=

# Load dataset
data(mtcars)

# Heatmap with color encoding for correlations
correlation_matrix <- cor(mtcars[, c("mpg", "wt", "hp", "qsec")])
correlation_data <- as.data.frame(as.table(correlation_matrix))
names(correlation_data) <- c("Variable1", "Variable2", "Correlation")

heatmap_plot1 <- ggplot(correlation_data, aes(x = Variable1, y = Variable2, fill = Correlation)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "#1E88E5", high = "#D81B60", mid = "white", midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  labs(
    title = "Real Colours",
    x = NULL,
    y = NULL
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

heatmap_plot2 <- ggplot(correlation_data, aes(x = Variable1, y = Variable2, fill = Correlation)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "#1E88E5", high = "#5D8DB7", mid = "white", midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  labs(
    title = "Perception of protan-type colour blind",
    x = NULL,
    y = NULL
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# Arrange plots in a grid
grid.arrange(heatmap_plot1, heatmap_plot2, ncol = 2)
@

\noindent \textbf{Avoiding Misleading Visualisations Due to Colour Choices}\\
Misleading visualisations often stem from inappropriate or deceptive use of colour, requiring precautions to prevent such occurrences. First, maintaining consistency in colour usage throughout the visualisation is essential. Employing a uniform colour scheme for similar data categories or elements helps establish coherence and understanding. Furthermore, it's crucial to avoid colour choices that could distort or exaggerate the data. Overly intense or contrasting colours might mislead interpretations, emphasising the necessity for judicious colour selection.\\

\noindent 
Additionally, providing a clear and concise legend becomes imperative to explain the meaning of colours, especially when dealing with complex or unfamiliar colour schemes. A comprehensive legend helps viewers decipher the represented data accurately.\\


\subsection{Cognitive Load and Visual Complexity}
In data visualisation, achieving a balance between complexity and cognitive load is crucial. Cognitive load significantly influences how viewers engage with and comprehend presented data. Finding a balance is crucial to effectively convey information without overwhelming the viewer's cognitive capacity. This section explores the concept of cognitive load in visualisations, strategies to reduce cognitive load while maintaining complexity, and techniques to combat information overload through simplification.

\subsubsection{Strategies to Reduce Cognitive Load While Maintaining Complexity}
To reduce cognitive load while maintaining complexity in data visualisation, several strategies can be employed. Firstly, establishing a clear \textbf{visual hierarchy} using size, color, and contrast helps direct attention to crucial elements. Additionally, simplifying \textbf{labels and text} by avoiding unnecessary complexity ensures information is clear and easily digestible.\\

\noindent Furthermore, employing \textbf{interactive features} like tooltips and drill-down functionality assists in providing additional information when required, reducing the density of static visualisations. A final approach involves the use of \textbf{progressive disclosure}, presenting complex information gradually, beginning with an overview and allowing users to explore details as needed.\\

<<cogload-plot, echo=FALSE, fig.height=3, fig.width=10, fig.cap='High versus low cognitive load demand through the reduction of visual complexity'>>=

# Create a cluttered plot with obscure axis labels and labels on all points
cluttered_plot <- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = as.numeric(row.names(iris)), shape = Species, label = rownames(iris))) +
  geom_point(size = 3) +
  geom_text(size = 2, vjust = 1.5, hjust = 1.5) +  # Add text labels
  labs(title = "High Visual Complexity",
       x = "sep.width",
       y = "sep.len",
       color = "Index",
       shape = "Species") +
  scale_color_gradient(low = "red", high = "purple") +  # Rainbow-colored points
  theme_minimal()

# Create a clear and simple plot
clear_plot <- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point(size = 2) +
  scale_color_manual(values = c("setosa" = "grey", "versicolor" = "grey", "virginica" = "#FDA638")) + 
  labs(title = "Low Visual Complexity",
       x = "Sepal Length",
       y = "Sepal Width",
       color = "Species") +
  theme_minimal()

# Use grid.arrange to arrange the plots side by side
grid.arrange(cluttered_plot, clear_plot, ncol = 2)
@

\subsubsection{Information Overload and Simplification Techniques}
Addressing information overload in visualisations necessitates the strategic application of simplification techniques. Filtering enables focused data selection, while data reduction aggregates information to highlight overarching trends. Storyboarding structures data presentation, aiding in contextual comprehension, and prioritisation ensures critical information is prominently displayed, elevating the visualisation's clarity and impact. These strategies collectively combat overwhelming data or excessive visual elements, enhancing comprehension and the effective communication of insights to viewers.

\newpage

\section{Univariate Data Visualisation Methods}

This chapter, initiates the discussion about data visualisation methods. This chapter particularly introduces techniques for graphically representing for datasets of the simplest nature - that is, single-variable datasets. It focuses on histograms, bar charts, Kernel Density Estimate, ROC Curves, and Time Series Analysis. 

\subsection{Histograms}

A histogram is a graphical representation of the distribution of a dataset. It consists of bars, each representing a range of data values, and whose height corresponds to the frequency or count of data points within that range.In this way, histograms facilitate the visualisation of differences in frequency or quantity among groups.\\

\noindent On a Cartesian coordinate system, the x-axis shows the endpoints of each group, and the y-axis represents frequency. The height of each rectangle indicates the corresponding frequency, making it a frequency distribution histogram. In order to determine the quantity of each group in the histogram, a multiplication of the frequency by the group interval is necessary. Since every histogram has a fixed group interval, if we use the y-axis to directly show quantity and each rectangle's height indicates the number of data points, we can both retain the distribution and simultaneously see the number in each group at a glance. All examples in this text use the non-standard histogram depiction with the y-axis denoting quantity.\\

\noindent
\textbf{Histograms in Practice}\\
\noindent The R language uses the \texttt{hist} function to create histograms. This function takes vectors as input, along with other parameters to plot a histogram.\\

\noindent Let \(X_1, \ldots, X_n\) be independent and identically distributed random variables with finite mean \( \mu \) and variance \( \sigma^2 \). Then for any \( \epsilon > 0 \), 
\[ P\left(\left|\frac{1}{n}\sum_{i=1}^n X_i - \mu\right| < \epsilon\right) \rightarrow 1 \] 
as \( n \rightarrow \infty \).\\

\noindent Histograms can be used to visualize the convergence of sample means to the population mean.\\

\noindent Let \(X_1, \ldots, X_n\) be independent and identically distributed random variables with finite mean \( \mu \) and variance \( \sigma^2 \). Define the sample mean \( \bar{X} = \frac{1}{n}\sum_{i=1}^n X_i \). Then, as \( n \) becomes large, the variable 
\[ Z = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \] approaches a standard normal distribution.\\

\noindent By plotting histograms of a large number of sample means, one can observe the shape gradually resembling a normal distribution.\\

\noindent
Consider the following graph with created on ggplot2 with the use of to the \texttt{geom\_histogram} function and iris dataset.

\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.48\linewidth}
<<hist2, echo=FALSE, fig.height=4, fig.width=5, out.width='\\linewidth'>>=
 # Create a histogram using ggplot2 for Sepal Length in the Iris dataset
bar_diagram <- ggplot(iris, aes(x = Sepal.Length)) +
  geom_histogram(
    binwidth = 0.2,  # Adjust the box width to 0.2 for smaller datasets
    fill = "#74c476"
  ) +
  labs(
    x = "Sepal Length (cm)",
    y = "Frequency"
  ) +
  theme_minimal()
print(bar_diagram)
@
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.48\linewidth}
<<kde3, echo=FALSE, fig.height=4, fig.width=5, out.width='\\linewidth'>>=
# Create a Kernel Density Estimate plot using ggplot2 for Sepal Length in the Iris dataset
kde_diagram <- ggplot(iris, aes(x = Sepal.Length)) +
  geom_density(
    fill = "#74c476",
    color = NA,
    alpha = 0.5,  # Adjust the transparency for better visualisation
    adjust = 1  # This parameter can be used to control the smoothness
  ) +
  labs(
    x = "Sepal Length (cm)",
    y = "Density"
  ) +
  theme_minimal()
print(kde_diagram)
@
  \end{minipage}
  \caption{Histogram and Kernel Density Estimation of Sepal Length in Iris Dataset}
  \label{fig:iris-plots}
\end{figure}

\subsection{Bar Charts}
\noindent Bar charts are a crucial tool in data presentation, arranging data into vertical or horizontal bars. The varying lengths of these bars directly correspond to the magnitude of the information they represent. Bar charts excel in comparing classified data, especially when values are closely aligned. This stems from the nature of human perception, as our visual acuity for height surpasses that of other visual elements like area or angle.\\

\noindent Bar charts represent a versatile tool for data visualisation, frequently employed to compare distinct categories. The \textbf{vertical bar chart}, commonly recognised, exhibits categories along the X-axis and their frequencies or counts along the Y-axis. \textbf{Horizontal bar charts}, rotated 90 degrees, prove beneficial for extended category names or numerous categories, displaying categories on the Y-axis and frequencies on the X-axis. \textbf{Multi-set or grouped bar charts} facilitate side-by-side comparisons of sub- groups within categories, available in both vertical and horizontal orientations. \textbf{Stacked bar charts} illustrate classes of values subdivided into sub-classes, often differentiated by colour, where each segment's size signifies its frequency or count, and the total bar length reflects the cumulative total.\\

\noindent \textbf{Bar Charts in Practice}\\
\noident The two bar charts on in Figure~\ref{fig:barcharts} illustrate the impact of varying vitamin dosages on tooth growth, further categorised by supplement type. On the X-axis, three distinct levels of vitamin dosage are presented, while the Y-axis indicates the average lenth of tooth for each dosage.


<<barcharts, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3, fig.width=10, fig.cap='Visualising tooth growth by gosage: A comparison of grouped and stacked bar chart techniques' >>=
>>>>>>> 152992594fdc38f065f2cd73424993608e2de712
# average length
average_lengths <- ToothGrowth %>%
  group_by(dose, supp) %>%
  summarize(average_len = mean(len))

barchart_plot1 <- ggplot(average_lengths, aes(x = factor(dose), y = average_len, fill = supp)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
  labs(x = "Dose (mg/day)", y = "Average Tooth Length (mm)", fill = "Supplement") +
  theme_minimal()
barchart_plot2 <- ggplot(average_lengths, aes(x = factor(dose), y = average_len, fill = supp)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(x = "Dose (mg/day)", y = "Tooth Length (mm)", fill = "Supplement") +
  theme_minimal()

ggsave("barcharts.pdf", plot = grid.arrange(barchart_plot1, barchart_plot2, ncol = 2), width = 10, height = 3)
@

\noindent From the grouped bar, chart found on the left of Figure~\ref{fig:barcharts}, due to the side-by-side positioning of the bars, it is easy to note that tooth growth varies not only with the dosage but also with the supplement type. In contrast, the stacked bar graph on the right of Figure~\ref{fig:barcharts} facilitates the understanding of the combined effects of the two supplements at each dosage level. However, compared to the latter, it becomes more challenging to differentiate the individual contribution of each supplement.

\subsection{Kernel Density Estimation}

Kernel Density Estimation (KDE) is an very useful tool in statistics. Instead of discrete histograms, it helps create a smooth curve from values in a dataset. KDE is used to infer the distribution of a population based on a limited sample. Thus, the result of the kernel density estimation is an estimate of the sample's probability density function. Based on this estimated probability density function, one can ascertain certain characteristics of the data distribution, such as the regions where data is concentrated.\\

\noindent
The KDE algorithm takes a parameter called bandwidth, that affects how smooth the resulting curve is. Changing the bandwidth changes the shape of the kernel: a lower bandwidth implies only points very close to the chosen position are given any weight, which leads to the estimate looking squiggly. In contrast, a higher bandwidth means having a shallow kernel, where distant points can contribute. Thus, leading to a smoother curve. %REVISE THIS TEXT.
\\

\noindent
We can express KDE as follows, where the K represent the kernel function:
$$\hat{f}(x) = \sum_{\text{observations}} K\left(\frac{x - \text{observation}}{\text{bandwidth}}\right).$$
In KDE, the variable $x$ represents the point of density estimation. The total number of data points, denoted as $n$, constitutes the sample size, and all these points are used for estimating the dataset's probability density function. The bandwidth $(h)$ is a crucial parameter that controls the smoothness of the estimation, with smaller bandwidths leading to closer fits to data and larger ones smoothing out details. The kernel function $(K)$, such as the Gaussian or triangular kernel, assigns mass around each data point, influencing the density estimate, while $x_i$ represents the individual sample points that collectively contribute to the estimated density function, guided by the kernel function and the bandwidth\cite{wand1994kernel}.
$$\hat{f}(x) = \frac{1}{nh} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right).$$\\
The kernel function \( K(u) \) is a normalised non-negative function that satisfies:
\[ \int K(u) \, du = 1. \]

\noindent In Figure~\ref{fig:iris-plots}, the kernel density estimation of the dataset is presented. The distribution of sepal lengths is depicted by the kernel density curve. The peaks observed in the curve correspond to the primary concentration trends of sepal length within the data. A unimodal curve signifies a concentration of sepal lengths for most irises in that specific region. Conversely, a bimodal or multimodal curve suggests the existence of multiple concentration areas.

\subsection{ROC Curve}

The Receiver Operating Characteristic (ROC) graph is a technique for assessing the performance of classification models, particularly useful for evaluating diagnostic tests and binary classifiers. 
It displays the performance of a binary classification system as its discrimination threshold is varied.\\

\noindent
ROC graph plots two parameters\cite{rocAnalysis}: True Positive Rate (TPR, also known as Sensitivity, is the proportion of positive instances correctly identified) and False Positive Rate (FPR, 1 - Specificity, is the proportion of negative instances incorrectly identified as positive). Also, the Area Under the Curve (AUC) provides a single measure of overall performance of a classifier. The larger the area under the curve, the better the classifier. An AUC of 0.5 suggests no discriminative ability, while an AUC of 1.0 represents perfect classification.\\

\noindent 
\textbf{ROC analysis in COVID-19 test}\\
The COVID-19 pandemic underscores the need for accurate diagnostic tests to differentiate between positive and negative cases\cite{Garcia2021ROCAlly}. An effective mean for evaluating accuracy of various diagnostic tests is the ROC analysis. ROC analysis comprehensively examines test accuracy by integrating TPR, FPR, and AUC. These metrics are calculated by comparing test outcomes against a recognized gold standard, thereby determining the true disease status. This approach is vital for understanding the efficacy of COVID-19 diagnostic tools and guiding public health decisions.\\

\noindent
The TPR, or sensitivity, measures the proportion of actual positives correctly identified, reflecting the test's ability to detect COVID-19 cases. The FPR, conversely, indicates the rate of false alarms, where non-infected individuals are wrongly identified as infected. An ideal test minimizes FPR, avoiding unnecessary treatments or quarantine. The ROC curve visualizes the trade-off between TPR and FPR at various thresholds. A curve arching towards the upper left indicates high sensitivity and low FPR — the hallmarks of a reliable test. The AUC consolidates the ROC performance into a single value: 0.5 denotes no better accuracy than random chance, while 1.0 represents perfect accuracy. A higher AUC indicates a better overall ability of the test.\\

\noindent
Here, the simulated data will be used to demonstrate the ROC analysis in COVID-19 test. Score\_Test1 and Score\_Test2 are the simulated test scores from two different diagnostic tests for COVID-19. These scores are continuous variables that typically would represent the likelihood of a positive diagnosis. The scores might come from a lab test result, such as a PCR test or a rapid antigen test, where higher scores indicate a higher likelihood of infection. The Condition column indicates the true condition of the patient, with 1 representing a positive COVID-19 case and 0 representing a negative case.

<<message=FALSE, echo=FALSE, warning=FALSE, fig.height=3.5, fig.width=3.5,fig.cap='ROC Curves for Two Diagnostic Tests'>>=
# Set seed for reproducibility
set.seed(0)

# Simulate scores for two diagnostic tests
scores_positive_test1 <- rnorm(100, mean = 0.75, sd = 0.2)
scores_negative_test1 <- rnorm(100, mean = 0.35, sd = 0.2)
scores_positive_test2 <- rnorm(100, mean = 0.7, sd = 0.2)
scores_negative_test2 <- rnorm(100, mean = 0.4, sd = 0.2)

# Combine the positive and negative scores
scores_test1 <- c(scores_positive_test1, scores_negative_test1)
scores_test2 <- c(scores_positive_test2, scores_negative_test2)

# Create a vector of true conditions (1 = positive, 0 = negative)
conditions <- c(rep(1, 100), rep(0, 100))

# Create ROC objects for both tests
roc_test1 <- roc(conditions, scores_test1)
roc_test2 <- roc(conditions, scores_test2)
@

<<roc, message=FALSE, warning=FALSE, fig.height=3.5, fig.width=3.5,fig.cap='ROC Curves for Two Diagnostic Tests'>>=
# Plot of ROC curves
plot(roc_test1, col="#1c61b6", lwd=2)
lines(roc_test2, col="#008600", lwd=2, lty=2)
legend("bottomright", legend=c("Diagnostic Test 1", "Diagnostic Test 2"),
       col=c("#1c61b6", "#008600"), lwd=2, lty=c(1, 2), cex=0.5)
# Add the perfect lines
abline(h=1, lty=2, col="red")
abline(v=1, lty=2, col="red") # Perfect Diagnostic Test (AUC = 1)
@

\noindent
From the plot~\ref{fig:roc}, the Diagnostic Test 1 has higher AUC. This test is likely to be more accurate in diagnosing COVID-19. It suggests that the test has a higher combined sensitivity and specificity, meaning it can identify positive cases more correctly and has fewer false alarms. Also, the Diagnostic Test 2 has a lower AUC than Test 1, This test is less accurate but still better than random guessing. It may miss more true cases (lower sensitivity) or incorrectly identify healthy individuals as having COVID-19 (higher false positive rate).\\

\noindent
In practice, the choice of which test to use may depend on other factors. For instance, if the cost of missing a COVID-19 case is high (for example, in a nursing home setting), a test with higher sensitivity might be preferred. Conversely, if the cost of false positives is high (for example, leading to unnecessary quarantine), a test with higher specificity might be favored.

\subsection{Time Series Analysis}

\noindent
Line charts are fundamental tools in data visualisation, particularly useful for displaying time series data. A line chart represents $n$ data points 
$\{(x_i,y_i)\}_{1 \leq i \leq n}$ on a Cartesian coordinate system, with the x-axis often denoting time intervals or ordered categories and the y-axis representing the measured values.\\ 

\noindent 
\textbf{Basics of Line Charts}\\
\noindent
In a line chart, consecutive data points are typically connected by straight lines. The line segment between two points \((x_i,y_i)\) and \((x_{i+1},y_{i+1})\) can be described by the equation of a line in the slope-intercept form: \(y=mx+b\), where \(m\) is the slope and \(b\) is the y-intercept. Also, a series of linear interpolations between pairs of data points could be used. These interpolations assume that the change between two points is uniform or linear. This linear approach is mathematically represented as:
\[
y = y_i + \frac{(y_{i+1} - y_i)}{(x_{i+1} - x_i)} \cdot (x - x_i), \quad  x_i \leq x \leq x_{i+1}.
\]
\noindent
This equation highlights that for any point \(x\) between \(x_i\) and \(x_{i+1}\), the corresponding value of \(y\) on the line chart is determined by a linear relation. This method effectively ``fills the gaps'' between actual observed data points and provides a continuous view of the data.

\subsubsection{Time Series}\\
\noindent
Time series visualisation is particularly suited to line charts. A time series is a collection of observations $x_t$, where $t$ denotes the time point at which the observation is recorded. An index set $T_0$ which collects all the time points when observations are available. For instance, we often have $T_0 = \{0,\dots ,n\}$ for $n \in \mathbb{N}$. By plotting these data points over time, line charts help in identifying long-term trends, seasonal patterns, and anomalies.\\

\noindent
A time series can be viewed as a realisation of a stochastic process. A stochastic process \cite{Brockwell2016Introduction} $X = (X_t)_{t \in T_0}$ is a collection of random variables $X_t$, where $t$ denotes the time index and $T_0$ the index set. For a fixed event $\omega \in \Omega$ we obtain the realisation of the stochastic process (sometimes also called a sample path) which is given by $x_t = X_t(\omega)$, $t \in T_0$. In practice, line charts of time series data provide insights into the behavior of such stochastic processes over time.\\

\noindent
\textbf{Time Series Visualisation of Exchange rates}\\
\noindent
Here, plot daily and 49-day moving average exchange rates of exchange rate data in one figure.
<<echo=FALSE,message=FALSE,warning=FALSE>>=
#Read in the data:
MyData <- read.csv("exchangeRate.csv",
                   header = TRUE, sep = ",",
                   dec = ".",
                   fileEncoding="UTF-8-BOM")

# Convert to data frame
MyData <- data.frame(MyData)

# Convert Date to a Date object
MyData$Date <- as.Date(MyData$Date, format="%d-%b-%y")


# Order the data by Date
MyData <- MyData[order(MyData$Date), ]


#ts_data <- zoo(MyData[-1], order.by = MyData$Date)
# Set an approximate frequency
#frequency(ts_data) <- 260
# Convert columns to time series
ts_data <- lapply(MyData[-1], ts,
                  start=c(as.numeric(format(min(MyData$Date), "%Y")),
                          as.numeric(format(min(MyData$Date), "%j"))),
                  end=c(as.numeric(format(max(MyData$Date), "%Y")),
                          as.numeric(format(max(MyData$Date), "%j"))),
                  frequency=260)

plot_dt <- MyData %>% select(Date, CANtoGBP, EURtoGBP, USDtoGBP) %>% gather(key="Currency", value="Rate", -Date) 

# 49-day moving average calculations and plot
columns <- names(MyData)[!names(MyData) %in% "Date"]
for (col in columns) {
  new_col_name <- paste0(col, "_MA49")
  MyData[[new_col_name]] <- zoo::rollapply(MyData[[col]],
                                           width=49, FUN=mean,
                                           fill=NA, align='right')
}

plot_data <- MyData %>% select(Date, CANtoGBP_MA49, EURtoGBP_MA49, USDtoGBP_MA49) %>% gather(key="Currency", value="Rate", -Date) %>%
  filter(grepl("MA49", Currency)) 

# Remove "_MA49" from the Currency column for the legend
plot_data$Currency <- gsub("_MA49", "", plot_data$Currency)
@
\begin{figure}[H]
<<message=FALSE,warning=FALSE,fig.dim = c(6, 3.5)>>=
# Plot daily and 49-day moving average exchange rates of CAN, EUR, USD to GBP

# First plot
p1 <- ggplot(plot_dt, aes(x=Date, y=Rate, color=Currency)) + geom_line() +
  labs(y="Exchange Rate to GBP", x = "", color="Currency")+
  theme_minimal() + theme(legend.position="none")
# Second plot
p2 <- ggplot(plot_data, aes(x=Date, y=Rate, color=Currency)) + geom_line() +
  labs(y="Exchange Rate to GBP", x = "", color="Currency")+
  theme_minimal() + theme(legend.position="none")
# Extract the legend
p2_legend <- cowplot::get_legend(p2 + theme(legend.position="right"))
# Combine the plots 
combined_plot <- cowplot::plot_grid(p1, p2, rel_widths = c(1, 1), nrow=2)
cowplot::plot_grid(combined_plot, p2_legend, nrow=1, rel_widths = c(2, 0.5))
@
\centering
\caption{Daily (top) and 49-day moving average (bottom) exchange rates of CAN, EUR, USD to GBP}
\label{fig:all exchange rates}
\end{figure}

\noindent
The first plot in Figure~\ref{fig:all exchange rates} presents a comparative visualisation of daily exchange rates for CAD, EUR, and USD against GBP, offering an overview of their trends and relative performance. This plot enables the identification of overall trends and periods of volatility for each currency pair, allowing for an assessment of their stability and strength relative to GBP. Notice that, there was a simultaneous and significant drop in all three currencies relative to the GBP. The concurrent nature of these declines across diverse currency pairs suggests that the driving factor is a depreciation of the GBP, rather than independent appreciations of the USD, EUR, and CAN. This notable depreciation of the GBP occurred around 2016, which coincides with the commencement of BREXIT process. Usually, the long term trend attracts financial analyst most. Therefore, filtering fluctuations and anomalies is important. Then, a 49-day moving average (MA49) was employed, shown in the second plot of Figure~\ref{fig:all exchange rates}, to elucidate long-term trends while mitigating short-term fluctuations. This is mathematically represented as 
\[\text{MA}_{49}(t) = \frac{1}{49} \sum_{k=t-48}^{t} x_k,\] 
where \( x_k \) denotes the exchange rate on day $k$.\\

\noindent
This method effectively filters out daily noise, allowing a clearer view of overarching trends in currency movements against the GBP. The overlay of these moving averages on the daily exchange rates in visualisations provides both a clear comparative and a quantitative perspective.\\

\noindent
\textbf{Decomposition of Time Series}\\
\noindent
One of the primary advantages of time series visualisation is the ease with which it allows analysts to identify long-term upward or downward trends in data and patterns that repeat over specific intervals. By decomposing the time series, it would be easy to see those features.\\

\noindent
Time series data, $X_t$, can often be described as a combination of several distinct components: Trend component $t_t$: The underlying progression in the series, Seasonal component $s_t$: Periodic fluctuations due to seasonal factor, Residual $r_t$: The irregular or error component.\\

\noindent
The decomposition of a time series can be described in two main models:\\
\textbf{Additive Model} \cite{Brockwell2016Introduction}: In the additive model, the components are added together:
\[
X_t = t_t + s_t + r_t.
\]
\textbf{Multiplicative Model} \cite{Brockwell2016Introduction}: In the multiplicative model, the components are multiplied together:
\[
X_t = t_t \times s_t \times r_t \quad \text{or} \quad \log(X_t) = \log(t_t) + \log(s_t) + \log(r_t).
\]
In practice, the choice between the additive and multiplicative models often depends on the nature of the time series. If the magnitude of the seasonal fluctuations or the variation around the trend does not vary with the level of the time series, then an additive model is appropriate. If the magnitude of the seasonal fluctuations or the variation around the trend increases or decreases as the time series level changes, then a multiplicative model may be more suitable.\\

\noindent
R function \texttt{decompose()} is able to decompose the time series by addictive model or multiplicative model. And below is the demonstration of decomposition.


<<message=FALSE,warning=FALSE,echo=FALSE>>=
# Plot decomposition of addictive time series model
decomposed_ts <- stats::decompose(ts_data$CNYtoGBP)

# Convert the decomposed object to a data frame
decomposed_df <- data.frame(
  time = rep(time(ts_data$CNYtoGBP), 4),
  value = c(decomposed_ts$x, decomposed_ts$trend, decomposed_ts$seasonal, decomposed_ts$random),
  component = factor(rep(c("Observed data", "Trend component", "Seasonal component", "Residual"), 
                         each = length(time(ts_data$CNYtoGBP))), 
                     levels = c("Observed data", "Trend component", "Seasonal component", "Residual")))
@

\begin{figure}[H]
<<message=FALSE,warning=FALSE,fig.dim = c(5, 4)>>=
# Plot decomposition of addictive time series model
ggplot(decomposed_df, aes(x = time, y = value)) + geom_line() +
  facet_wrap(~component, scales = "free_y", ncol = 1) + labs(x = "Date",y = "Exchange Rate to GBP") + theme_minimal()
@
\centering
\caption{Decomposition of addictive time series model of CNY to GBP exchange rates}
\label{fig:decomposition of time series}
\end{figure}

\noindent
From Figure~\ref{fig:decomposition of time series}, the CNY to GBP exchange rate time series was decomposed into its fundamental components: trend, seasonality, and residual noise by additive model. This additive model, represented mathematically as $X_t = t_t + s_t + r_t$.\\

\noindent
As illustrated in Figure~\ref{fig:decomposition of time series}, the trend component $t_t$ of CNY to GBP exchange rate exhibits a distinct pattern over time: initially, it shows a gradual decrease and reached crest in 2022, followed by a sudden increase afterwards. It coincides with the tax reduction policy issued by UK government in 2022, which leads to a depreciation of GBP. This trend is pivotal for understanding the broader economic relationship between these currencies. \\

\noindent
Moreover, the seasonal component $s_t$ of the decomposition highlights cyclical fluctuations, indicative of recurrent patterns within the year. These could be attributed to seasonal economic activities, policy changes, or other cyclical factors influencing the currency market. The clear demarcation of these cyclical trends in the seasonal component helps in isolating such effects from the overarching trend.\\

\noindent
Lastly, the residual component $r_t$ encompasses the random, unexplained variations after accounting for the trend and seasonal factors. analysing these residuals is crucial for understanding the unpredictability in the exchange rate and can be pivotal in risk management and forecasting.\\

\noindent
\textbf{Autocorrelation Analysis of CNY to GBP Exchange Rate}\\
\noindent
Autocorrelation, also referred to as serial correlation, is a crucial concept in time series analysis. It describes the correlation of a time series with its own past and future values. The autocorrelation function (ACF) measures the linear predictability of the series at lag h, which is the time $t$ with its values at a previous time $t-h$. The mathematical fomulation of ACF of time series will be given in the following paragraph.\\

\noindent
Suppose we have a time series with observations denoted by \( x_1, \ldots, x_n \). Then the sample mean is given by $\bar{x} = \frac{1}{n} \sum_{t=1}^{n} x_t.$
And the sample autocovariance function at lag $h$ in days of our time series is
\[\hat{\gamma}(h) := \frac{1}{n} \sum_{t=1}^{n-|h|} (x_{t+|h|} - \bar{x})(x_t - \bar{x}), \quad -n < h < n \cite{Brockwell2016Introduction}.\]
Hence, the sample autocorrelation function at lag $h$ in days is given by
\[\hat{\rho}(h) := \frac{\hat{\gamma}(h)}{\hat{\gamma}(0)} = \frac{\sum_{t=1}^{n-|h|} (x_{t+|h|} - \bar{x})(x_t - \bar{x})}{\sum_{t=1}^{n} (x_t - \bar{x})^{2}}, \quad -n < h < n \cite{Brockwell2016Introduction}.\]
\noindent
The value of $\hat{\rho}(h)$ lies between -1 and +1. A value close to +1 indicates a strong positive correlation, while a value close to -1 indicates a strong negative correlation. A value near 0 suggests little to no linear correlation. A slow decay in the ACF plot indicates a strong relationship between past and present values, while spikes at specific lags may suggest seasonality. Autocorrelations outside the 95\% confidence interval are considered statistically significant.\\

\noindent
Next, visualise the ACF for the CNY to GBP exchange rate to understand its time-dependent structure better.

\begin{figure}[H]
<<echo=TRUE, message=FALSE, warning=FALSE, fig.dim = c(4, 3)>>=
# Plot the Autocorrelation Function (ACF)
acf_data <- acf(MyData$CNYtoGBP, plot = FALSE)
# Plot using base R plotting
plot(acf_data, main="", xlab="Lag h", ylab="ACF")
@
\centering
\caption{Autocorrelation Function at lag h in days of CNY to GBP Exchange Rate}
\label{fig:ACF}
\end{figure}

\noindent
From Figure~\ref{fig:ACF}, the ACF plot for the CNY to GBP exchange rate series reveals that the ACF starts near 1 and decreases gradually. This pattern suggests a strong persistence in the time series, indicating that past values have a significant influence on future values. In time series analysis, such a slow decay in the ACF is indicative of a non-stationary series, where the mean, variance, and autocorrelation structure do not remain constant over time.\\

\noindent
This persistent autocorrelation suggests that short-term movements in the CNY to GBP exchange rate are heavily influenced by its recent history. Such a characteristic is crucial for forecasting models, as it implies that recent historical data can be a powerful predictor of near-future trends. Models like ARIMA (Autoregressive Integrated Moving Average), which are well-suited for data with high autocorrelation, may be particularly effective in this context.


\newpage
\section{Bivariate Data Visualisation Methods}


\subsection{Heatmaps}
The heatmap is a data visualisation technique that uses colour coding to represent different intensities. It can be represented as an $m \times n$ matrix $\mathbf{M}$, with $m$ observations for variable 1 and $n$ observations for variable 2:

$$\mathbf{M} =
\left[
\begin{array}{cccc}
    \mathrm{M}_{11} & \mathrm{M}_{12} & \ldots & \mathrm{M}_{1n} \\  
    \mathrm{M}_{21} & \mathrm{M}_{22} & \ldots & \mathrm{M}_{2n} \\  
    \vdots & \vdots & \ddots & \vdots \\  
    \mathrm{M}_{m1} & \mathrm{M}_{m2} & \ldots & \mathrm{M}_{mn}
\end{array}
\right],
$$

\noindent
where each entry $\mathrm{M}_{mn}$ represent an observation.\\

\noindent
In this illustrative example, heatmaps are used to visualise fire occurrences in Brazil. These heatmaps provide a spatially coherent representation, highlighting regions at high risk and seasonal patterns. The data-driven insights could empower policymakers to make informed decisions regarding preventive measures and firefighting strategies.\\

\noindent
In Figure~\ref{fig:fire22}, it can be observed that significantly higher fire counts are found in certain locations. The presence of two strips with high frequencies of fires are highly unusual. The vertical trend corresponds to the location of BR-230 (Trans-Amazonian Highway) passing through the city of Apuí, State of Amazonas. The horizontal trend corresponds to BR-163 (Brazil highway) passing through Três Pinheiros in Novo Progresso, State of Pará. The western coastal area with a high frequency of fire occurrence corresponds to regions in close proximity to the cities of Vista Alegre do Abunã and Rio Branco. Research has indicated that 95 \% of active fires and the most intense ones (FRP $>$ 500 megawatts) occurred at the edges in forests \cite{forest}.\\

\noindent
The seasonal pattern of fire frequency is shown in Figure~\ref{fig:fire-by-months-fy13-22}. Observe that more fire occur in the months of August to October compared to the rest of the year.

<<load-data, echo=FALSE,message=FALSE,warning=FALSE>>=
# Data from NASA: https://firms.modaps.eosdis.nasa.gov/
brazil_fire_fy13 <- read.csv("fire_data/modis_2013_Brazil.csv")
brazil_fire_fy14 <- read.csv("fire_data/modis_2014_Brazil.csv")
brazil_fire_fy15 <- read.csv("fire_data/modis_2015_Brazil.csv")
brazil_fire_fy16 <- read.csv("fire_data/modis_2016_Brazil.csv")
brazil_fire_fy17 <- read.csv("fire_data/modis_2017_Brazil.csv")
brazil_fire_fy18 <- read.csv("fire_data/modis_2018_Brazil.csv")
brazil_fire_fy19 <- read.csv("fire_data/modis_2019_Brazil.csv")
brazil_fire_fy20 <- read.csv("fire_data/modis_2020_Brazil.csv")
brazil_fire_fy21 <- read.csv("fire_data/modis_2021_Brazil.csv")
brazil_fire_fy22 <- read.csv("fire_data/modis_2022_Brazil.csv")
@


<<data-cleaning, echo=FALSE,message=FALSE,warning=FALSE>>=
data_list <- list(
  brazil_fire_fy13,
  brazil_fire_fy14,
  brazil_fire_fy15,
  brazil_fire_fy16,
  brazil_fire_fy17,
  brazil_fire_fy18,
  brazil_fire_fy19,
  brazil_fire_fy20,
  brazil_fire_fy21,
  brazil_fire_fy22)

# Function to filter confident fire observation
filter_fire <- function(data) {
  filter_fire <- data %>% filter(confidence >= 95)
  return(filter_fire)
}

# Apply the filtering function to all data frames in the list
confident_fire_decade <- lapply(data_list, filter_fire)

# Access filtered data for a specific fiscal year, fy22
confident_fire_fy22 <- confident_fire_decade[[10]]
@

<<pivot-data-by-month, echo=FALSE,message=FALSE,warning=FALSE>>=
# Create a pivot table, no. of fire occurrences vs. Months (Jan-Dec), in FY22.
confident_fire_months_fy22 <- confident_fire_fy22 %>%
  mutate(acq_date = as.Date(acq_date, format = "%Y-%m-%d")) %>%
  group_by(month = floor_date(acq_date, 'month')) %>%
  summarize(count = n())

# change "2022-01-01" to Jan etc.
confident_fire_months_fy22$abb_month <- format(confident_fire_months_fy22$month, "%b")

# Create a custom order for the months
custom_order <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

# Reorder the "abb_month" factor based on the custom order
confident_fire_months_fy22$abb_month <- factor(confident_fire_months_fy22$abb_month, levels = custom_order)
@


\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.48\linewidth}
<<spacetime-fy22,fig.height=6,fig.show='hold', echo=FALSE, message=FALSE, warning=FALSE>>=
# Obtain the Brazil map data
brazil_map <- map_data("world", region = "Brazil")

# Create the heatmap of fire occurrences
space_heatmap <- ggplot(confident_fire_fy22, aes(x = longitude, y = latitude)) +
  geom_polygon(data = brazil_map, aes(x = long, y = lat, group = group), 
               fill = "#bdbdbd") +
  geom_bin2d(bins = 300) +
  scale_fill_gradient(low = "#fee6ce", high = "#d7301f") +
  coord_fixed(ratio = 1) +
  theme_minimal()+
  theme(axis.text = element_text(size = 9))

print(space_heatmap)
@
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.48\linewidth}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{image_reference/mapshoot.png}
\end{figure}
  \end{minipage}
  \caption{Frequency of fire in Brazil (2022), two strips of high frequencies of fires are highly unusual}
  \label{fig:fire22}
\end{figure}

<<pivot-table-fy13-22, echo=FALSE,message=FALSE,warning=FALSE>>=
# Combine all the confident fire datasets into a single dataset
combined_confident_fire <- bind_rows(confident_fire_decade)

# Create a pivot table for fy13-22
pivot_table <- combined_confident_fire %>%
  mutate(acq_date = as.Date(acq_date, format = "%Y-%m-%d"),
         month_year = format(acq_date, "%b %Y")) %>%
  group_by(month_year) %>%
  summarize(count = n())

# Add column "abb_month" and "year" to the dataset "pivot_table"
pivot_table <- pivot_table %>%
  mutate(abb_month = gsub(".*?([A-Za-z]{3}).*", "\\1", month_year)) %>%
  mutate(abb_month = factor(abb_month, levels = custom_order)) %>%
  mutate(year = as.numeric(substring(month_year, nchar(month_year) - 3, nchar(month_year))))
@

<<fire-by-months-fy13-22,fig.height=2.6,fig.width=5,fig.cap='Frequency of fire in Brazil (2013-2022)',fig.show='hold', message=FALSE,warning=FALSE>>=
heatmap_plot <- ggplot(pivot_table, 
                       aes(x = factor(abb_month, levels = custom_order), 
                           y = as.character(year), fill = count)) +
  geom_tile() +
  scale_fill_gradient(low = "#fff7ec", high = "#d7301f") +
  labs(x = " ", y = " ") +
  theme_minimal() +
  theme(axis.text = element_text(size = 9))

print(heatmap_plot)
@


\subsection{Scatter Plots}

A scatter plot is a graphical representation of a set of data points in a two-dimensional coordinate system. Each data point is represented by a dot, and the position of the dot is determined by the values of two variables. Some examples of this type of visualisation can be found in Figure~\ref{fig:abs-plots}, Figure~\ref{fig:cogload-plot}, and Figure~\ref{fig:bubble-plot-construction}.\\

\noindent
In general, \(Y\) denotes the response, or dependent,variable and \(X\) denotes the explanatory, or indenpendent,  variable. Let \((x_i, y_i)\) represent the coordinates of the \(i\)-th data point on the scatter plot. The scatter plot can be mathematically described as a set of points: \( \{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\}, \) where \(n\) is the number of observations in the set.

\subsection{Bubble Charts}
Bubble charts are a captivating data visualisation tool that extends beyond the typical two-dimensional scatter plot by introducing an extra dimension. They represent data points as bubbles or circles on a two-dimensional plane, where the size of each bubble encodes a third variable.\\

\noindent The construction of bubble charts, is parallel to that of a scatter plot, but involves the scaling the data values to determine the size of each bubble. While it isn't alwaysthe case, the size of these is typically proportional to the variable it represents. The choice of scaling method depends on the data distribution and the message the chart aims to convey.\\

\noindent Generally, the formula for calculating the bubble size (\(S\)) involves applying the scaling function
\[
S = k \cdot V,
\]
\noindent where \(S\) represents the size of the bubble, \(V\) the value of the variable being represented, and \(k\) a scaling factor to control the bubble size. Selecting an appropriate scaling factor (\(k\)) is critical for maintaining the proportionality between the bubble size and the variable being represented.\\

\noindent
\textbf{Bubble Charts in Practice}\\
The bubble chart shown in Figure~\ref{fig:bubble-plot} visualises data from the mtcars dataset. It depicts the relationship between car models and their fuel efficiency (mpg) while using the size of the bubbles to represent the car's horsepower (hp) and color-coding the bubbles based on the number of cylinders (cyl). Hence, making it a multivariate data visualisation.\\

<<bubble-plot, echo=FALSE, fig.height=5, fig.width=11, fig.cap='Bubble plot illustrating trends in car types and performance', message=FALSE>>=
ggplot(mtcars, aes(x = rownames(mtcars), y = mpg, size = hp, color = cyl)) +
  geom_point() +
  labs(
    x = "Car Models",
    y = "Miles per Gallon",
    size = "Horsepower (hp)",
    color = "Cylinders (cyl)"
  ) +
  scale_size_continuous(range = c(3, 10)) +
  scale_color_gradient(low = "lightblue", high = "darkblue") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust =1))
@

\\\noindent In Figure~\ref{fig:bubble-plot} a strong correlation between the cars' number of cylinder and their horsepower can be quickly identified. This is due to the fact that as size of the dots (representing horsepower) increases, their colour (representing the number of cylinders) simultaneously becomes darker. Furthermore, a correlation between the miles per gallon consumed by the different types of cars and their number of cylinders and horsepower is easily identifiable in Figure~\ref{fig:bubble-plot}. This is shown by the fact that in the same way that the darker dots cluster towards the bottom of the graph and become lighter as they reach the top of the graph, those with a large diameter also cluster near the lower part and decrease in size as they ascend. \\ 

\noindent 
The efficiency of bubble charts is highlighted by the fact that to capture just some of the information provided by this single bubble chat - that is, the information regarding the car models, the miles covered per gallon, and the horsepower of each of these, one would require three different scatter plots. These are displayed in Figure~\ref{fig:bubble-plot-construction}. The contrast between the simplicity and readability of Figure~\ref{fig:bubble-plot}, and the density and complexity of Figure~\ref{fig:bubble-plot-construction} emphasises the advantages of bubble charts in representing datasets with a higher number of variables. 

<<bubble-plot-construction, echo=FALSE, fig.height=4, fig.width=11, fig.cap='Scatter plot deconstruction of Figure 16', message=FALSE>>=

# Scatter plot 1: Car models vs. Miles per gallon
plot1 <- ggplot(mtcars, aes(x = rownames(mtcars), y = mpg)) +
  geom_point(color = "blue", size =2) +
  labs(x = "Car Models", y = "Miles per Gallon", title = "Car Models vs. MPG") +
  theme_bw() +
  theme(axis.text.x = element_blank(),
        panel.border = element_blank())

# Scatter plot 2: Car models vs. Horsepower
plot2 <- ggplot(mtcars, aes(x = rownames(mtcars), y = hp)) +
  geom_point(color = "darkblue", size =2) +
  labs(x = "Car Models", y = "Horsepower", title = "Car Models vs. Horsepower") +
  theme_bw() +
  theme(axis.text.x = element_blank(),
        panel.border = element_blank())

# Scatter plot 3: Miles per gallon vs. Horsepower
plot3 <- ggplot(mtcars, aes(x = mpg, y = hp)) +
  geom_point(color = "purple", size =2) +
  labs(x = "Miles per Gallon", y = "Horsepower", title = "MPG vs. Horsepower") +
  theme_bw() +
  theme(panel.border = element_blank())

# Arrange plots side by side
grid.arrange(plot1, plot2, plot3, ncol = 3)
@


\subsection{Simple Linear Regression}
\\Regression models are statistical tools that provide functions to estimate the relationship between the response variable and one or more explanatory variables. Regression analysis is widely adopted by data scientists, who use large datasets to build predictive models for trend forecasting. The fol- lowing paragraphs will introduce simple linear regression models and demonstrate their usage using the mtcars dataset.

\subsubsection{Theory of Simple Linear Regression} 

\\Let $\mathbf{x} = (x_1 \dots x_n)^T$ denote $n$ explanatory variables and let $\mathbf{Y} = (Y_1 \dots Y_n)^T$ denote $n$ corresponding response variables.
\\  
\\In a simple linear model, it is assumed that the response variables $Y_1 \dots Y_n$ are uncorrelated with a common variance $\sigma^2$, and their expectations are given by $E(Y_i| x_i) = \beta_0 + \beta_1 x_i$. The expectations generated by $\beta_0$ and $\beta_1$ given $x_i$ can be expressed as:

\begin{equation}\label{eq:4-1}
\mathrm{E}(\mathbf{Y} | \mathbf{x}) =
\left( \begin{array}{ccc}
\beta_0+\beta_1 x_1\\
\vdots\\
\beta_0+\beta_1 x_n
\end{array} \right) = 
\left( \begin{array}{ccc}
1\\
\vdots\\
1
\end{array} \right) \beta_0 + 
\left( \begin{array}{ccc}
x_1\\
\vdots\\
x_n
\end{array} \right) \beta_1 =
\mathbf{1}_n \beta_0 + \mathbf{x} \beta_1 ,
\end{equation}
\\
\noindent
where $\mathbf{1}_n$ is an n-vector of 1's.
\\  
\\Given design matrix $\mathbf{X}$ where $X_i = (1, x_i)$ and $\beta = (\beta_0, \beta_1)^T$, then $E(Y_i|x_i) = X_i \beta$. These assumptions can be equivalently written in the vector form:

\begin{equation}\label{eq:4-2}
\mathrm{E}(\mathbf{Y} | \mathbf{x}) = 
\left( \begin{array}{cc}
1 & x_1\\
\vdots& \vdots\\
1 & x_n
\end{array} \right) 
\left( \begin{array}{cc}
\beta_0 \\
\beta_1
\end{array} \right) = \mathbf{X} \beta, \quad 
\text{var}(\mathbf{Y} | \mathbf{x}) =
\begin{pmatrix}
\sigma^2 & 0 & \cdots & 0 \\
0 & \sigma^2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma^2
\end{pmatrix} = \sigma^2 \mathbf{I}_n.
\end{equation}

\subsubsection{Least Squares Estimation}

\noindent The residual sum of squares (RSS) is a measure of the goodness of fit in a regression model, where residuals are the differences between the response variables $y_i$ and responses generated by the regression model $\mathrm{E}(\mathbf{Y}_i | \mathbf{x})$. In least squares estimation, the goal is to find values of parameters $\mathbf{\beta} = (\beta_0, \beta_1)^T$ to minimise the RSS, denoted by $\mathrm{Q}$: 

\begin{equation}\label{eq:4-3}
\mathrm{Q} = \sum_{i=1}^{n} [y_i - \mathrm{E} (Y_i | \mathbf{x})]^2 
           = [\mathbf{y}- \mathrm{E} (\mathbf{Y} | \mathbf{x})]^{T} [\mathbf{y}- \mathrm{E} (\mathbf{Y} | \mathbf{x})] 
           = [\mathbf{y}- \mathbf{X} \beta]^{T} [\mathbf{y}- \mathbf{X} \beta],
\end{equation}

\noindent
where $\mathbf{y}$ is n-vector of response variables and $\mathbf{X}$ is the $n \times 2$ design matrix. The partial derivative of $Q$ with respect to vector $\beta$ is:

\begin{equation}\label{eq:4-4}
\frac{\partial Q}{\partial \beta} = 2(\mathbf{X}^T\mathbf{X}\beta - \mathbf{X}^T\mathbf{y}),
\end{equation}

\noindent
Equating $\frac{\partial Q}{\partial \beta} = \mathbf{0}$, the vector $\hat{\beta}$, the least squares estimate of $\beta$, can be written as:

\begin{equation}\label{eq:4-5}
\mathbf{X}^T(\mathbf{y}-\mathbf{X}\hat{\beta})=\mathbf{0}.
\end{equation}

\noindent 
The least squares estimate of $\beta$ is given by:
\begin{equation}\label{eq:4-6}
\hat{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.
\end{equation}
\\ 
\noident 
\\\textbf{Case example: 1970s automobiles}\\
\noindent In this section, we will study the performance of 1970s automobiles using the mtcars dataset, employing the method of linear regression. Performance is measured in Miles per Gallon (mpg); the higher the mileage, the more efficient the automobile. We will start with the visualisation of a simple linear regression model, followed by the discussion of linear regression models and the model selection method.

\noindent
\\In the preliminary stages of data exploration, calculating the correlation matrix is a crucial step before engaging in regression modeling, as shown in Figure~\ref{fig:cor-matrix-mtcars2}. In real-world scenarios, variables are often correlated, and entirely independent relationships are seldom encountered. Therefore, analysing pairwise correlations becomes essential. This helps in understanding multicollinearity issues within the model. Multicollinearity occurs when one covariate within the model can be accurately predicted from another covariate. When this happens, the coefficient estimates of the model can change unpredictably due to minor changes in the data.\\

<<cor-matrix-mtcars,echo=FALSE,message=FALSE,warning=FALSE>>=
correlation_matrix <- cor(mtcars)
rounded_correlation_matrix <- round(correlation_matrix, 3)
# Reshape the correlation matrix for ggplot
correlation_matrix <- as.data.frame(as.table(rounded_correlation_matrix))
@

<<cor-matrix-mtcars2, echo=FALSE,message=FALSE,warning=FALSE,fig.cap='Correlation matrix of all variables in mtcars dataset',fig.height=2.9,fig.width=5>>=
# Create a heatmap using ggplot2
correlation_matrix <- ggplot(correlation_matrix, aes(Var1, Var2, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 0.5, size = 2) +  # Display numbers inside the grid
  scale_fill_gradient2(low = "#542788", mid = "white", high = "#b2182b", 
                       midpoint = 0, limits = c(-1, 1), space = "Lab",
                       name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
  labs(x = " ",y = " ")
print(correlation_matrix)
@

\noindent
For the simple linear regression model, the response variable is Miles per Gallon (mpg), and we select weight (wt) as the explanatory variable. Note that mpg and wt are highly correlated, with a correlation coefficient of -0.868. This suggests that wt may have strong predictive power for mpg. Use the R function \texttt{lm()} to calculate the linear regression model, with the summary displayed below.\\

\noindent
Observe that the t-test yields a p-value of $1.29 \times 10^{-10}$, which is less than $0.001$. This indicates that the variable wt holds high statistical significance in this model. For the fitted model, the slope is $\beta_1 = -5.3445$, meaning that for every increase of 1000 lbs, the car efficiency decreases by 5 miles per gallon. The RSS is 3.046. The simple linear regression line is displayed in Figure~\ref{fig:scatter-plot}.

<<modelwt, echo=TRUE,message=FALSE,warning=FALSE>>=
Modelwt <- lm(formula = mpg ~ wt, data = mtcars)
summary(Modelwt)
@

<<scatter-plot, echo=FALSE, fig.height=2.6, fig.width=4, fig.cap='Scatter plot of car weights vs MPG', message=FALSE>>=
# Create a scatter plot of car weight vs MPG from mtcars dataset
library(ggplot2)
ggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +
  geom_point(size = 2, alpha = 0.7) +  # Larger points with transparency
  geom_smooth(method = "lm", se = FALSE, color = "#b2182b", linewidth = 1) +  # Linear regression trend line
  labs(
    x = "Weight (1000 lbs)",
    y = "Miles per Gallon",
    color = "Cylinders"
  ) +
  scale_color_manual(values = c("4" = "grey", "6" = "#542788", "8" = "blue")) +  # Custom color palette
  theme_minimal() # Minimalistic theme
@

\subsection{LOESS Regression}
\noindent
Locally weighted scatterplot smoothing (LOWESS) is a local regression method for a response variable $Y$ on a single predictor $X$. LOWESS was proposed by William S. Cleveland in 1979, it is the special case to higher dimention locally estimated scatterplot smoothing (LOESS) regression method. They are non-parametric regression methods that fits a linear regression model for each neighbourhood of data. Unlike GLM, the LOESS model does not provide a global function to fit the data; rather, it fits neighborhoods of the data.\\

\noindent
\subsubsection{Theory of LOESS regression}\\

\noindent
Suppose we have $n$ ordered observations $(x_1,y_1) \dots (x_n,y_n)$ with predictors $x_i$ and response variables $y_i$. Assume a model of the form

$$y_i = g(x_i) +  \varepsilon_i,$$
\noindent
where $g(\cdot)$ is an unknown smooth function and $\varepsilon_i$ is i.i.d. Gaussian error terms with mean $0$ and variance $\sigma^2$.\\

\noindent
Let $\Delta_i(x) = |x - x_i|$ represent the horizontal distance between $x$ and $x_i$. Let $\Delta_{(i)}(x)$ denote the ordered distances to $x$, arranged from smallest to largest.\\

\noindent
For each point $(x,y)$ consider the neighbourhood to fit a line segment around it. The size of the neighbourhood indicates the number of data points used to fit the line segments and is determined by the smoothing parameter $\alpha$. For $\alpha \leq 1$, each  neighbourhood consists of $\frac{n}{\alpha}$ number of points \cite{rloess}. Each point $(x_i, y_i)$ in the neighbourhood of point $(x,y)$ is assigned with a weight, ``importance" of the data point to the line segment \cite{ytloess}. The weight of $(x_i,y_i)$ is denoted as $w_i(x)$, which is defined as:

\begin{equation}\label{eq:4-7}
w_i(x) = T(\Delta_i (x); \Delta _{(q)}(x)).
\end{equation}

\noindent
Tricube weight function $T(u,t)$ is defined as:
\begin{equation}\label{eq:4-8}
T(u;t) = \left\{
  \begin{array}{ll}
    (1-(u/t)^3)^3 & \text{for } 0 \leq u < t \\
    0 & \text{for } u \geq t \\
  \end{array}
\right, 
\end{equation}

\noindent
where $u$ is the horizontal distance between neighbourhood point of interest $(x_i,y_i)$ to point $(x,y)$ and $t$ is the maximum distance threshold. For points in the neighbourhood with horizontal distance exceeding threshold $t$, they are assigned with weight $0$ by the tricube weight function.\\

\noindent
For $\alpha > 1$, for each point $(x,y)$ the neighbourhood include all points. The maximum distance threshold is assumed to be $\alpha^{\frac{1}{p}}$, for $p$ explanatory variables. The weight is given by:

\begin{equation}\label{eq:4-9}
w_i(x) = T(\Delta_i (x); \Delta _{(n)}(x))\alpha.\\
\end{equation}

\noindent
The line segment is fitted by weighted least square, and this is the preliminary estimation $\hat{g}(x)$ for every point $(x,y)$.\\

\noindent
Both LOWESS and LOESS penalise the outliers in the preliminary estimation by assigning points further away from $\hat{g}(x)$ with less weight than before. The robustness weight (the adjusted weight) is given by:
$$r_i = B(\hat{\varepsilon}_i, 6\mathrm{m}),$$

\noindent
where bisquare weight function is defined as:

\begin{equation}\label{eq:4-10}
B(u;b) = \left\{
  \begin{array}{ll}
    (1-(u/b)^2)^2 & \text{for } 0 \leq |u| < b \\
    0 & \text{for } |u| \geq b \\
  \end{array},
\right
\end{equation}

\noindent
and the median absolute residual $\mathrm{m}$ is defined as $\mathrm{m} = \mathrm{median}(|\hat{\varepsilon}_i|)$, the residual $\hat{\varepsilon}_i$ is defined as $\hat{\varepsilon}_i = y_i - \hat{y}(x_i)$. \\

\noindent
Note the process of weight adjustment using previous estimation is iterated several times until a smooth curve is obtained.\\

\noindent
The updated model estimate $\hat{g}(x)$, is computed using the local fitting method, but with the neighborhood weights $w_i{(x)$ replaced by $r_i w_i(x)$ \cite{loess}.\\

\noindent
\textbf{Case example: Taiwan Housing dataset}\\

\noindent
Property valuation can be modelled as a regression problem, in this analysis, we delve into the pricing of properties based on their age. Using the Taipei Housing dataset, we investigate the relationship between the age of houses (measured in years) and their prices (measured in New Taiwan dollars per unit area). Linear regression model and the LOESS regression model are used, as shown in Figure~\ref{fig:loess-plot}.\\

\noindent
The linear model has a residual standard error of 13.32, while the LOESS model has a residual standard error of 12.16. Cross-validation is a method to compare model goodness of fit. LOESS, with its ability to capture local variations may outperform linear regression, which minimises the least squares estimate $\hat{\beta}$.\\

<<estate-data-1, echo=FALSE,message=FALSE,warning=FALSE>>=
estate <- read.csv("RealEstate.csv")
@

<<estate-data-2, echo=FALSE,message=FALSE,warning=FALSE>>=
# Specify custom variable names
var_names <- c("No","transaction_date", "house_age", "dist_to_station", "no_of_stores", "lat","long","price_per_area")

# Read the CSV file with custom variable names
estate <- read.csv("/Users/margaretli/Downloads/realestate.csv", header = FALSE, col.names = var_names)
# Delete the first row of the data frame
estate <- estate[-1, ]

estate$house_age <- as.numeric(estate$house_age)
estate$price_per_area <- as.numeric(estate$price_per_area)
@

<<estate-data-plot,fig.height=2.5,fig.width=3,fig.cap='Estate valuation using house age, linear regression and smooth regression methods', message=FALSE,warning=FALSE>>=
ggplot(data = estate, aes(x = house_age, y = price_per_area)) +
  geom_point(size =0.5) +
  theme_minimal()+
  scale_x_continuous(labels = scales::number_format(scale = 1)) +
  scale_y_continuous(labels = scales::number_format(scale = 1)) +
  geom_smooth(method = "loess", se = FALSE, color = "#bd0026", span = 0.5) + 
  geom_smooth(method = "lm", se = FALSE, color = "#0868ac", formula = y ~ x) +
  labs(title = "House Age vs. House Price", 
       x = "house age (year)", 
       y = "price (unit area)") +
  theme_minimal()
@


\newpage

\section{Visualising Beyond Two Dimensions}

\subsection{Multiple Linear Regression}

\noindent
\textbf{Linear Regression Model and Model selection method AIC}
\\  
\noindent 
\\Model selection methods, such as the Akaike Information Criterion (AIC), play a crucial role in statistical modeling. AIC provides an estimate of the information lost when a given model is used to represent the process that generated the data, it is calculated as follows: 

$$\mathrm{AIC} = -2 \l(\hat{\boldsymbol {\theta}})+2 \dim (\boldsymbol{\theta}) \cite{aic},$$
\\  
\\where $\l(\hat{\boldsymbol{\theta}})$ is the log-likelihood function, let $\l(\hat{\boldsymbol{\theta}}) = 0$ to find the Maximum Likelihood Estimator $\hat{\boldsymbol{\theta}}$ of a distribution.\\

\noindent 
The R algorithm \texttt{step()}, which minimises the AIC score, aids in selecting the most appropriate regression model from a set of explanatory variables. This algorithm uses backward selection method. Starting with all explanatory variables, the algorithm iteratively excludes one covariate at each step, until statistically significant reduction in AIC is achieved. This algorithm aims to find the best-fitting model by balancing goodness of fit and simplicity. Note that \texttt{step} provides comparison between competing models rather than an absolute measure of model quality.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{image_reference/codeshoot.png}
    \label{fig:codeshoot}
\end{figure}
\noindent
The procedure for selecting the best linear model for the mtcars dataset is shown in Appendix A, where our best-fitted model is \texttt{formula = mpg \textasciitilde wt + qsec + am}. Given 10 explanatory variables, Miles per Gallon is best predicted by weight (wt), quarter-mile time (qsec), and Transmission (am), where 0 represents automatic and 1 represents manual.


\subsection{PCA}

Visualizing and analyzing data with many variables can be challenging due to the complexity and high dimensionality. Dimensionality reduction techniques like Principal Component Analysis (PCA) simplify this by transforming the data into a lower-dimensional space (typically 2D or 3D), making it more accessible and manageable. PCA, a linear method, is especially effective for datasets with linear relationships, identifying the principal components that maximize variance and preserve significant features of the original data. This not only aids in visualization but also enhances computational efficiency and helps mitigate the curse of dimensionality.\\

\noindent
For a collection of $N$ high-dimensional objects $\mathbf{X_1},\dots,\mathbf{X_N}$, where each $\mathbf{X_i}$ is a vector in a D-dimensional space. The entire dataset can be represented as a matrix $X \in \mathbb{R}^{N \times D}$, where each row corresponds to one of the N objects.\\

\noindent
Before applying PCA, it is crucial to standardize the dataset because it ensures that each feature contributes equally to the analysis. Without standardization, features with larger scales dominate the variance, potentially skewing the PCA results. For $1 \leq i \leq N$ and $1 \leq j \leq D$, the standardized value $Z_{ij}$ for each data point $X_{ij}$ is calculated as:
    \[Z_{ij} = \frac{X_{ij} - \mu_j}{\sigma_j},\]
where $X_{ij}$ is the original value, $\mu_j$ is the mean of feature $j$, and $\sigma_j$ is the standard deviation of feature $j$. The result is a standardized dataset $\mathbf{Z} = \{\mathbf{Z_1}, \dots, \mathbf{Z_N}\}$ with $N$ objects, each represented in a $D$-dimensional standardized space. Then, the covariance matrix is $\mathbf{\Sigma}$ for the standardized dataset $\mathbf{Z}$. The covariance matrix is a $D \times D$ matrix, calculated as:
    \[\mathbf{\Sigma} = \frac{1}{N-1} \mathbf{Z}^T \mathbf{Z}.\]
Since $\mathbf{Z}$ is standardized, this matrix reflects the covariance between each pair of features. Therefore, the eigen decomposition of the covariance matrix $\Sigma$ is given by:
    \[\mathbf{\Sigma} \mathbf{v} = \lambda \mathbf{v},\]
where each eigenvector $\mathbf{v}$ and its corresponding eigenvalue $\lambda$ satisfy this equation. The eigen decomposition of the covariance matrix $\mathbf{\Sigma}$ reveals the principal components of the data. Each eigenvector represents a direction in the feature space along which the data is variably distributed, and the corresponding eigenvalue indicates the variance of its eigenvector. Importantly, these eigenvectors are orthogonal to each other, underscoring the identification of uncorrelated principal components in the data. Then, order the eigenvectors by their corresponding eigenvalues in descending order and select the top $k$ eigenvectors $\{\mathbf{v_1}, \ldots, \mathbf{v_k}\}$, which correspond to the largest $k$ eigenvalues. These form the columns of the matrix $\mathbf{W} \in \mathbb{R}^{D \times k}$.\\

\noindent
Finally, project the standardized data $\mathbf{Z}$ onto the subspace defined by the principal components:
    \[\mathbf{Y} = \mathbf{Z}\mathbf{W},\]
where $\mathbf{Y} \in \mathbb{R}^{N \times k}$ is the transformed data matrix in the new subspace. Hence, in the reduced space, the variance along each principal component $\mathbf{v_i}$ is maximized, preserving the most significant variances in the data. PCA also attempts to preserve pairwise Euclidean distances between data points in the lower-dimensional representation.    

\subsection{Biplots}
\noindent 
Biplots are designed visualise high-dimensional data by projecting it onto a lower-dimensional space while retaining essential information. They are commonly understood to be a generalisation of the simple two-variable scatterplot.\\

\noindent At their core, biplots offer a graphical representation of a standardised data matrix, whose rows \(n\) are the samples, and whose columns \(p\) are the variables, by projecting these onto a two-dimensional plane. To do this, mathematical computations derived from techniques like Singular Value Decomposition (SVD) or Principal Component Analysis (PCA) are required. SVD is particularly essential in the generation of biplots as it breaks down the data matrix into three constituent matrices, allowing for the extraction of singular values and singular vectors — which form the basis for plotting the data in the reduced-dimensional space.

\subsubsection{Construction of Biplots}

\noindent As we will see in detail subsequently, the steps to generate a biplot involve:
\begin{itemize}
    \item \textbf{Data Standardisation}: Standardising the data by centering and scaling it to ensure each variable contributes equally.
    \item \textbf{Calculating Singular Value Decomposition (SVD)}: Applying SVD to the standardised data matrix to obtain the singular values and vectors.
    \item \textbf{Projection}: Projecting both the samples and variables onto a two-dimensional space using the singular vectors obtained from SVD. 
\end{itemize}

\noindent \textbf{The Data Matrix}\\
\noindent Data matrices used to produce biplots can take 3 different forms. They are most commonly composed by cases $\times$ variables (where the cases are always assumed to be the row and the variables be the columns). But data matrices can also be composed by categories $\times$ categories (a cross tabulation), or by cases $\times$ categories (compositional data), and more.\\

\noindent \textbf{Data Standardisation}\\
Depending on the initial data matrix $\mathbf{M}$, various transformations may be necessary to arrived at the standardised data matrix $\mathbf{H}$. These may include functional transformations like logarithmic, sine, power, etc., as well as matrix transformations such as observed/expected frequency ratio, odds ratio, log ratio, etc. Additionally, transformations may involve applying weights to rows and/or columns, (weighted) centering of rows and columns, double-centering, and normalization of rows and columns.\\

\noindent \textbf {(Weighted) Singular Value Decomposition (SVD)}\\
\noindent (Weighted) Singular Value Decomposition (SVD) is performed via the eigendecomposition of a cross-product matrix whose order depends on the number of columns of $\mathbf{H}$, since the number of columns is usually less than the number of rows.\\

\noindent The most general situation is that in which there are weights $r$ for the rows and weights $c$ for the columns, with associated diagonal matrices $\mathbf{D}_r$ and $\mathbf{D}_c$. Defaults are $\mathbf{D}_r = (1/n)\mathbf{I}$ (each of the $n$ rows is weighted equally by $1/n$) and $\mathbf{D}_c = \mathbf{I}$.\\

\noindent The weighted SVD is obtained by decomposing the matrix \(D_r^{1/2} Z D_c^{1/2} = U\Gamma V^T\), where \(\Gamma\) is the diagonal matrix of eigenvalues. Alternatively, if proceeding via the weighted eigendecomposition, \(D_c^{1/2} Z^T D_r Z D_c^{1/2} = V\Gamma^2 V^T\) where \(\Gamma_2\) is the diagonal matrix of eigenvalues, followed by the transformation to the left vectors: \(U = D_r^{1/2} Z D_c^{1/2} V\Gamma^{-1}\).\\

\noindent \textbf{Projection: Computation of Coordinates}\\
\noindent There are three types of coordinates. Firstly, the most common, there's the \textbf{standard coordinates} whose rows are given by $F_s = D^{-1/2}_r U$, and the columns by $G_s = D^{-1/2}_c V$. There's also the \textbf{principal coordinates} which have rows given by $F_p = Fs\Gamma$ and columns given by $G_p = Gs\Gamma$. Finally, there's the \textbf{“canonical” coordinates} which have rows given by $F_c = Fs\Gamma^{1/2}$ and columns $G_c = Gs\Gamma^{1/2}$.\\

\noindent Usual choices of coordinates give the following standard types of biplots:
\begin{itemize}
  \item Asymmetric map (form biplot) of the rows: plot $F_p$ and $G_s$.
  \item Asymmetric map (covariance biplot) of the columns: plot $F_s$ and $G_p$.
  \item Symmetric or canonical biplot: plot $F_c$ and $G_c$.
\end{itemize} \\

!!!!!!!!!!!!!!!!!!\\

\noindent In the context of PCA-based biplots, the primary steps involve determining principal components from the covariance or correlation matrix of the data. The first two principal components, capturing the most significant variation, form the basis of the biplot. In this graphical representation, observations are depicted as points, while variables are represented as vectors emanating from the origin. The angles and lengths of these vectors signify relationships between variables and their contributions to the principal components, while the proximity of points reflects similarity or dissimilarity between observations.\\

\noindent Mathematically, the coordinates of observations on the first two principal components can be calculated using formulas derived from PCA:

$$\[
x_{i1} = s_{i1} \cdot u_{1}
\]
\[
x_{i2} = s_{i2} \cdot u_{2},
\]

\noindent where \( x_{i1} \) and \( x_{i2} \) are the coordinates of observation \( i \) on the first and second principal components respectively, \( s_{i1} \) and \( s_{i2} \) are the scores of observation \( i \) on these components, and \( u_{1} \) and \( u_{2} \) are the unit eigenvectors corresponding to the first and second principal components.\\

\noindent Similarly, the coordinates of the variable vectors are computed using the following formulas:

\[
v_{j1} = \sqrt{\lambda_{1}} \cdot c_{j1}
\]
\[
v_{j2} = \sqrt{\lambda_{2}} \cdot c_{j2}.
\]

\noindent Here, \( v_{j1} \) and \( v_{j2} \) represent the coordinates of variable \( j \) on the first and second principal components respectively, \( \lambda_{1} \) and \( \lambda_{2} \) are the eigenvalues corresponding to the first and second principal components, and \( c_{j1} \) and \( c_{j2} \) are the loadings of variable \( j \) on these components.\\

<<PCAbiplot, fig.height=7, fig.width=10, fig.cap='PCA Biplot of mtcars Dataset: Principal Component Analysis', message=FALSE>>=
data(mtcars)
#Generation of a PCA Biplot 

# Standardise the data
scaled_data <- scale(mtcars)

# Perform Principal Component Analysis (PCA)
pca_result <- prcomp(scaled_data, scale. = TRUE)

# Create a biplot
biplot(pca_result, cex = 0.7)
@

\subsection{t-SNE}
https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf

\noindent
Since PCA is a linear method and may not perform well with non-linear data structures, often missing complex non-linear relationships. Although effective in capturing the global structure of data, PCA focuses on maintaining large pairwise distances to maximize variance and overlook important local patterns and structures. \\

\noindent
Unlike PCA, t-Distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear technique for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions, focusing on preserving the small pairwise similarities between data points thus retain the local structure of the dataset in a lower-dimensional space. It starts by converting high-dimensional Euclidean distances into probabilities that reflect the similarity between points, then maps these points to a lower-dimensional space in a way that tries to preserve these similarities.\\

\noindent
First, for each pair of high-dimensional points $x_i$ and $x_j$, the similarity between data points is quantified using conditional probabilities $p_{j|i}$, which is calculated using a Gaussian distribution centered at $x_i$:
    \begin{equation}
        p_{j|i} = \frac{\exp(-||\mathbf{x}_i - \mathbf{x}_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i}\exp(-||\mathbf{x}_i - \mathbf{x}_k||^2 / 2\sigma_i^2)}
    \end{equation}\\

\noindent
Then, the joint probability distribution $P$ in the high-dimensional space is symmetrized as follows:
    \begin{equation}
        p_{ij} = \frac{p_{j|i} + p_{i|j}}{2N}
    \end{equation}\\

\noindent
In the low-dimensional space, t-SNE alleviates the crowding problem, which is a common issue in methods like PCA (Principal Component Analysis). It does so by using a t-distribution in the low-dimensional space, which has heavier tails than a Gaussian distribution. t-SNE calculates similar probabilities using a Student's t-distribution:
    \begin{equation}
        q_{ij} = \frac{(1 + ||\mathbf{y}_i - \mathbf{y}_j||^2)^{-1}}{\sum_{k \neq l}(1 + ||\mathbf{y}_k - \mathbf{y}_l||^2)^{-1}}
    \end{equation}\\
    
\noindent
The objective is to minimize the Kullback-Leibler divergence between the joint probability distributions $P$ and $Q$:
    \begin{equation}
        C = \text{KL}(P || Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
    \end{equation}
This process is achieved through gradient descent. The gradient of symmetric SNE is fairly similar to that of asymmetric SNE, and is given 
\begin{equation}
        \partial C = \text{KL}(P || Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
    \end{equation}
    
\noindent
Perplexity influences the variance of the Gaussians in the high-dimensional space and is a crucial hyperparameter in t-SNE.\\


\noindent
R provides us with a dedicated package for t-SNE analysis, \textit{Rtsne}, which allows us to conduct the analysis conveniently using the \textit{Rtsne} function. In the graph presented below, we observe a t-SNE visualization of the penguin dataset.\\
<<t-SNE, echo=FALSE, fig.height=3, fig.width=5, fig.cap='t-SNE of Penguins Dataset', message=FALSE>>=
penguins %>% 
  select(where(is.numeric),-year,species) %>%
  mutate(ID=row_number()) %>% 
  column_to_rownames("ID") %>% 
  na.omit()-> df

tSNE_fit<-df %>% 
  select(-species) %>% 
  scale() %>% 
  Rtsne()
tSNE_fit$Y %>% 
  as.data.frame() %>% 
  rename(tSNE1="V1",
         tSNE2="V2") %>% 
  mutate(Species=df$species) -> tSNE.plot
library(ggplot2)
ggplot()+
  geom_point(data=tSNE.plot,
             aes(x=tSNE1,y=tSNE2,color=Species))+
  stat_ellipse(data=tSNE.plot,
               geom="polygon",
               aes(x=tSNE1,y=tSNE2,
                   group=Species,
                   fill=Species),
               alpha=0.5,
               lty="dashed",
               color="black",
               key_glyph="blank")+
  theme_bw()
@

\noindent
The provided plot in Figure~\ref{fig:t-SNE} is a t-SNE visualization of the Penguins dataset, rendered using the $ggplot2$ package in R. In this case, using the T-sne algorithm helps us to reduce the multidimensional numerical data of the penguin dataset to a $2$-dimensional space,and we also using scatter plots and elliptical areas  to visualize the distribution of different penguin. The data points are color-coded to distinguish between three penguin species: Adelie in red, Chinstrap in green, and Gentoo in blue. This color coding aids in the visual differentiation of the species clusters. The axes are labeled $tSNE1$ and $tSNE2$, corresponding to the dimensions reduced by the t-SNE algorithm. Dashed ellipses overlaid on the scatter plot demarcate the clusters of each species, providing a visual guide to the density and separation of the species within the transformed feature space. The plot effectively uses the t-SNE technique to illustrate the grouping of species, highlighting the algorithm's utility in discerning inherent data patterns in a lower-dimensional representation. \\


\noindent
While t-SNE is an exceptionally potent tool for data visualization, it does come with its own set of constraints. Firstly, it has a significant memory footprint and can be time-consuming to run, which may pose challenges when dealing with large datasets. Secondly, it is tailored specifically for visualization, which constrains the embedding space to two or three dimensions. This limitation means that t-SNE is optimized for human interpretability rather than for capturing higher-dimensional relationships. Additionally, it requires experimentation with different initializations to mitigate the risk of local suboptimal solutions affecting the results. Therefore, it's crucial to carefully consider and select the most suitable method based on the specific requirements and nature of the data at hand.\\
\newpage  

\section{Index}  

\begin{itemize}
    \item \textbf{Accuracy}: Emphasising faithful reflection of data to reduce distortion or misinterpretation.
    \item \textbf{Clarity}: Ensuring visuals are easily understood without unnecessary complexity.
    \item \textbf{Cognitive Load}: The mental effort required to process information presented visually.
    \item \textbf{Cognitive psychology}: Understanding how mental processes affect perception and understanding of visual data.
    \item \textbf{Color Theory}: Principles guiding the strategic use of color in visualisations for clarity and impact.
    \item \textbf{Colour encoding}: Utilising colors to represent data categories or values in visualisations.
    \item \textbf{Colour perception}: Understanding how humans perceive and interpret colors in visualisations.
    \item \textbf{Consistency}: Maintaining uniform use of visual elements throughout a visualisation.
    \item \textbf{Data abstraction}: Simplifying and structuring raw data into comprehensible visual forms.
    \item \textbf{Gestalt Principles}: Rules affecting how visual elements are grouped and interpreted in perception.
    \item \textbf{Hierarchies of abstraction}: Representing data at various levels of detail in visualisation.
    \item \textbf{Information overload}: Occurs when excessive data or visual elements overwhelm understanding.
    \item \textbf{Interval data}: Ordered categories with equal intervals but lacking a true zero point.
    \item \textbf{Nominal data}: Represents categories or labels without any inherent order.
    \item \textbf{Ordinal data}: Implies a meaningful order among categories but lacks equal intervals.
    \item \textbf{Ratio data}: Includes ordered categories with equal intervals and a meaningful zero point.
    \item \textbf{Relevance}: Presenting information pertinent to the addressed message or question.
\end{itemize}
  
  

\newpage

\bibliographystyle{plain} % Choose a style that suits your needs
\bibliography{reference} % The filename of your .bib file
 
\newpage

\appendix
\section{Appendix A}

\begin{verbatim}
Start:  AIC=70.9
mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb

       Df Sum of Sq    RSS    AIC
- cyl   1    0.0799 147.57 68.915
- vs    1    0.1601 147.66 68.932
- carb  1    0.4067 147.90 68.986
- gear  1    1.3531 148.85 69.190
- drat  1    1.6270 149.12 69.249
- disp  1    3.9167 151.41 69.736
- hp    1    6.8399 154.33 70.348
- qsec  1    8.8641 156.36 70.765
<none>              147.49 70.898
- am    1   10.5467 158.04 71.108
- wt    1   27.0144 174.51 74.280

Step:  AIC=68.92
mpg ~ disp + hp + drat + wt + qsec + vs + am + gear + carb

       Df Sum of Sq    RSS    AIC
- vs    1    0.2685 147.84 66.973
- carb  1    0.5201 148.09 67.028
- gear  1    1.8211 149.40 67.308
- drat  1    1.9826 149.56 67.342
- disp  1    3.9009 151.47 67.750
- hp    1    7.3632 154.94 68.473
<none>              147.57 68.915
- qsec  1   10.0933 157.67 69.032
- am    1   11.8359 159.41 69.384
- wt    1   27.0280 174.60 72.297

Step:  AIC=66.97
mpg ~ disp + hp + drat + wt + qsec + am + gear + carb

       Df Sum of Sq    RSS    AIC
- carb  1    0.6855 148.53 65.121
- gear  1    2.1437 149.99 65.434
- drat  1    2.2139 150.06 65.449
- disp  1    3.6467 151.49 65.753
- hp    1    7.1060 154.95 66.475
<none>              147.84 66.973
- am    1   11.5694 159.41 67.384
- qsec  1   15.6830 163.53 68.200
- wt    1   27.3799 175.22 70.410

Step:  AIC=65.12
mpg ~ disp + hp + drat + wt + qsec + am + gear

       Df Sum of Sq    RSS    AIC
- gear  1     1.565 150.09 63.457
- drat  1     1.932 150.46 63.535
<none>              148.53 65.121
- disp  1    10.110 158.64 65.229
- am    1    12.323 160.85 65.672
- hp    1    14.826 163.35 66.166
- qsec  1    26.408 174.94 68.358
- wt    1    69.127 217.66 75.350

Step:  AIC=63.46
mpg ~ disp + hp + drat + wt + qsec + am

       Df Sum of Sq    RSS    AIC
- drat  1     3.345 153.44 62.162
- disp  1     8.545 158.64 63.229
<none>              150.09 63.457
- hp    1    13.285 163.38 64.171
- am    1    20.036 170.13 65.466
- qsec  1    25.574 175.67 66.491
- wt    1    67.572 217.66 73.351

Step:  AIC=62.16
mpg ~ disp + hp + wt + qsec + am

       Df Sum of Sq    RSS    AIC
- disp  1     6.629 160.07 61.515
<none>              153.44 62.162
- hp    1    12.572 166.01 62.682
- qsec  1    26.470 179.91 65.255
- am    1    32.198 185.63 66.258
- wt    1    69.043 222.48 72.051

Step:  AIC=61.52
mpg ~ hp + wt + qsec + am

       Df Sum of Sq    RSS    AIC
- hp    1     9.219 169.29 61.307
<none>              160.07 61.515
- qsec  1    20.225 180.29 63.323
- am    1    25.993 186.06 64.331
- wt    1    78.494 238.56 72.284

Step:  AIC=61.31
mpg ~ wt + qsec + am

       Df Sum of Sq    RSS    AIC
<none>              169.29 61.307
- am    1    26.178 195.46 63.908
- qsec  1   109.034 278.32 75.217
- wt    1   183.347 352.63 82.790

Call:
lm(formula = mpg ~ wt + qsec + am, data = mtcars)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.4811 -1.5555 -0.7257  1.4110  4.6610 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   9.6178     6.9596   1.382 0.177915    
wt           -3.9165     0.7112  -5.507 6.95e-06 ***
qsec          1.2259     0.2887   4.247 0.000216 ***
am            2.9358     1.4109   2.081 0.046716 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.459 on 28 degrees of freedom
Multiple R-squared:  0.8497,	Adjusted R-squared:  0.8336 
F-statistic: 52.75 on 3 and 28 DF,  p-value: 1.21e-11
\end{verbatim}
\end{document}